{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Data PDF Processing Pipeline\n",
    "\n",
    "This notebook processes FBI crime statistic PDFs through the following steps:\n",
    "1. Download PDFs from S3\n",
    "2. Convert PDFs to markdown using Mistral OCR\n",
    "3. Upload generated markdown to S3\n",
    "4. Extract year information\n",
    "5. Create chunks for embedding\n",
    "6. Upload chunks to S3\n",
    "7. Create vector embeddings in Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, uuid, shutil, tempfile, re, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Custom modules - update these paths if needed:\n",
    "from mistralparsing_userpdf import process_pdf as mistral_process_pdf\n",
    "from utils.chunking import KamradtModifiedChunker\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Year Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_from_filename(filename):\n",
    "    \"\"\"Extract a 4-digit year from a filename.\"\"\"\n",
    "    # Look for 4 consecutive digits that likely represent a year (between 1900 and 2099)\n",
    "    match = re.search(r'(?:19|20)\\d{2}', filename)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None  # Return None if no year found\n",
    "\n",
    "def extract_year_from_content(content):\n",
    "    \"\"\"Extract year from markdown content based on common patterns.\"\"\"\n",
    "    # Pattern for \"FBI Releases YYYY Crime Statistics\" or similar\n",
    "    title_match = re.search(r'FBI Releases (\\d{4}) Crime Statistics', content)\n",
    "    if title_match:\n",
    "        return title_match.group(1)\n",
    "        \n",
    "    # Look for years in the text that are likely report years\n",
    "    year_matches = re.finditer(r'(?:in|for|during|of)(?: the)? (?:year )?(\\d{4})', content.lower())\n",
    "    for match in year_matches:\n",
    "        year = match.group(1)\n",
    "        # Validate year is between 1900 and current year\n",
    "        if 1900 <= int(year) <= datetime.now().year:\n",
    "            return year\n",
    "            \n",
    "    # Last resort: just find any 4-digit number that looks like a year\n",
    "    general_year = re.search(r'\\b((?:19|20)\\d{2})\\b', content)\n",
    "    if general_year:\n",
    "        return general_year.group(1)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 and Pinecone Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- S3 Setup ---\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=os.getenv(\"AWS_SERVER_PUBLIC_KEY\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SERVER_SECRET_KEY\")\n",
    ")\n",
    "bucket = os.getenv(\"BUCKET_NAME\")\n",
    "input_folder = \"crime records summary/\"  # The folder with PDFs in S3\n",
    "markdown_folder = \"processed_markdown/\"  # Where to store processed markdown files\n",
    "chunks_folder = \"chunks/\"  # Where to store chunks\n",
    "\n",
    "# --- Pinecone Setup ---\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "# Delete index if it exists\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "# Create new index with ServerlessSpec\n",
    "pc.create_index(\n",
    "    name=index_name, \n",
    "    dimension=384, \n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"  # Adjust region as needed\n",
    "    )\n",
    ")\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# --- Temp Dir ---\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"pdf_downloads_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download PDFs from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PDFs from S3...\n",
      "Downloaded: 1995Summary.pdf\n",
      "Downloaded: 1996Summary.pdf\n",
      "Downloaded: 1997Summary.pdf\n",
      "Downloaded: 1998Summary.pdf\n",
      "Downloaded: 1999Summary.pdf\n",
      "Downloaded: 2000Summary.pdf\n",
      "Downloaded: 2001Summary.pdf\n",
      "Downloaded: 2002Summary.pdf\n",
      "Downloaded: 2003Summary.pdf\n",
      "Downloaded: 2004Summary.pdf\n",
      "Downloaded: 2006Summary.pdf\n",
      "Downloaded: 2007 CIUS Summary.pdf\n",
      "Downloaded: 2008 CIUS Summary.pdf\n",
      "Downloaded: 2009Summary.pdf\n",
      "Downloaded: 2010 CIUS Summary.pdf\n",
      "Downloaded: 2011Summary.pdf\n",
      "Downloaded: 2012 CIUS Summary.pdf\n",
      "Downloaded: 2013 CIUS Summary _final.pdf\n",
      "Downloaded: 2014 CIUS Summary_final.pdf\n",
      "Downloaded: 2015 CIUS Summary_final.pdf\n",
      "Downloaded: 2016 CIUS Summary.pdf\n",
      "Downloaded: 2017 CIUS Summary.pdf\n",
      "Downloaded: 2018 CIUS Summary.pdf\n",
      "Downloaded: 2019 CIUS Summary.pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading PDFs from S3...\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=input_folder)\n",
    "pdf_paths = []\n",
    "original_filenames = {}  # To keep track of original filenames for year extraction\n",
    "\n",
    "for obj in resp.get(\"Contents\", []):\n",
    "    if obj[\"Key\"].endswith(\".pdf\"):\n",
    "        filename = os.path.basename(obj[\"Key\"])\n",
    "        local_pdf = os.path.join(temp_dir, filename)\n",
    "        s3.download_file(bucket, obj[\"Key\"], local_pdf)\n",
    "        pdf_paths.append(local_pdf)\n",
    "        original_filenames[local_pdf] = filename  \n",
    "        print(f\"Downloaded: {filename}\")\n",
    "\n",
    "if not pdf_paths:\n",
    "    print(\"No PDFs found in the S3 folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert PDFs to Markdown and  Save Markdown File Paths for Later Processing\n",
    "\n",
    "We'll save the markdown file paths to a JSON file so we can process them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting PDFs to Markdown...\n",
      "Processing 1995Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\1995Summary.pdf\n",
      "Processing 1995Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 1995.md\n",
      "Uploading markdown to S3: processed_markdown/1995.md\n",
      "Processing 1996Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\1996Summary.pdf\n",
      "Processing 1996Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 1996.md\n",
      "Uploading markdown to S3: processed_markdown/1996.md\n",
      "Processing 1997Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\1997Summary.pdf\n",
      "Processing 1997Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 1997.md\n",
      "Uploading markdown to S3: processed_markdown/1997.md\n",
      "Processing 1998Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\1998Summary.pdf\n",
      "Processing 1998Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 1998.md\n",
      "Uploading markdown to S3: processed_markdown/1998.md\n",
      "Processing 1999Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\1999Summary.pdf\n",
      "Processing 1999Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 1999.md\n",
      "Uploading markdown to S3: processed_markdown/1999.md\n",
      "Processing 2000Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2000Summary.pdf\n",
      "Processing 2000Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2000.md\n",
      "Uploading markdown to S3: processed_markdown/2000.md\n",
      "Processing 2001Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2001Summary.pdf\n",
      "Processing 2001Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2001.md\n",
      "Uploading markdown to S3: processed_markdown/2001.md\n",
      "Processing 2002Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2002Summary.pdf\n",
      "Processing 2002Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2002.md\n",
      "Uploading markdown to S3: processed_markdown/2002.md\n",
      "Processing 2003Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2003Summary.pdf\n",
      "Processing 2003Summary.pdf ...\n",
      "Error processing PDF: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Error markdown saved to C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2003.md\n",
      "Uploading markdown to S3: processed_markdown/2003.md\n",
      "Processing 2004Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2004Summary.pdf\n",
      "Processing 2004Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2004.md\n",
      "Uploading markdown to S3: processed_markdown/2004.md\n",
      "Processing 2006Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2006Summary.pdf\n",
      "Processing 2006Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2006.md\n",
      "Uploading markdown to S3: processed_markdown/2006.md\n",
      "Processing 2007 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2007 CIUS Summary.pdf\n",
      "Processing 2007 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2007.md\n",
      "Uploading markdown to S3: processed_markdown/2007.md\n",
      "Processing 2008 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2008 CIUS Summary.pdf\n",
      "Processing 2008 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2008.md\n",
      "Uploading markdown to S3: processed_markdown/2008.md\n",
      "Processing 2009Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2009Summary.pdf\n",
      "Processing 2009Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2009.md\n",
      "Uploading markdown to S3: processed_markdown/2009.md\n",
      "Processing 2010 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2010 CIUS Summary.pdf\n",
      "Processing 2010 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2010.md\n",
      "Uploading markdown to S3: processed_markdown/2010.md\n",
      "Processing 2011Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2011Summary.pdf\n",
      "Processing 2011Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2011.md\n",
      "Uploading markdown to S3: processed_markdown/2011.md\n",
      "Processing 2012 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2012 CIUS Summary.pdf\n",
      "Processing 2012 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2012.md\n",
      "Uploading markdown to S3: processed_markdown/2012.md\n",
      "Processing 2013 CIUS Summary _final.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2013 CIUS Summary _final.pdf\n",
      "Processing 2013 CIUS Summary _final.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2013.md\n",
      "Uploading markdown to S3: processed_markdown/2013.md\n",
      "Processing 2014 CIUS Summary_final.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2014 CIUS Summary_final.pdf\n",
      "Processing 2014 CIUS Summary_final.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2014.md\n",
      "Uploading markdown to S3: processed_markdown/2014.md\n",
      "Processing 2015 CIUS Summary_final.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2015 CIUS Summary_final.pdf\n",
      "Processing 2015 CIUS Summary_final.pdf ...\n",
      "Error processing PDF: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Error markdown saved to C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2015.md\n",
      "Uploading markdown to S3: processed_markdown/2015.md\n",
      "Processing 2016 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2016 CIUS Summary.pdf\n",
      "Processing 2016 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2016.md\n",
      "Uploading markdown to S3: processed_markdown/2016.md\n",
      "Processing 2017 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2017 CIUS Summary.pdf\n",
      "Processing 2017 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2017.md\n",
      "Uploading markdown to S3: processed_markdown/2017.md\n",
      "Processing 2018 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2018 CIUS Summary.pdf\n",
      "Processing 2018 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2018.md\n",
      "Uploading markdown to S3: processed_markdown/2018.md\n",
      "Processing 2019 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2019 CIUS Summary.pdf\n",
      "Processing 2019 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2019.md\n",
      "Uploading markdown to S3: processed_markdown/2019.md\n",
      "Successfully generated 24 markdown files\n"
     ]
    }
   ],
   "source": [
    "# --- Convert PDFs to Markdown (Mistral) ---\n",
    "print(\"Converting PDFs to Markdown...\")\n",
    "md_files = []\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    try:\n",
    "        pdf_filename = os.path.basename(pdf_path)\n",
    "        print(f\"Processing {pdf_filename} ...\")\n",
    "        \n",
    "        # First extract year from PDF filename\n",
    "        year = extract_year_from_filename(pdf_filename)\n",
    "        if not year:\n",
    "            print(f\"Warning: Could not extract year from filename {pdf_filename}\")\n",
    "            # Use a timestamp if no year is found\n",
    "            year = datetime.now().strftime('%Y')\n",
    "            \n",
    "        out_dir = os.path.join(temp_dir, \"mistral_output\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        # Verify the PDF file exists\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "            \n",
    "        # Get absolute path to ensure no path issues\n",
    "        abs_pdf_path = os.path.abspath(pdf_path)\n",
    "        print(f\"Using absolute PDF path: {abs_pdf_path}\")\n",
    "        \n",
    "        # Process the PDF - this always outputs to output.md\n",
    "        md_path = mistral_process_pdf(pdf_path=abs_pdf_path, output_dir=out_dir)\n",
    "        \n",
    "        if os.path.exists(md_path):\n",
    "            # Create a simplified markdown filename using just the year\n",
    "            simplified_md_filename = f\"{year}.md\"\n",
    "            simplified_md_path = os.path.join(out_dir, simplified_md_filename)\n",
    "            \n",
    "            print(f\"Renaming output to simplified filename: {simplified_md_filename}\")\n",
    "            \n",
    "            # Copy the content to the simplified filename\n",
    "            with open(md_path, 'r', encoding='utf-8') as src:\n",
    "                content = src.read()\n",
    "                \n",
    "            with open(simplified_md_path, 'w', encoding='utf-8') as dest:\n",
    "                dest.write(content)\n",
    "                \n",
    "            # Add the simplified path to our list\n",
    "            md_files.append(simplified_md_path)\n",
    "            \n",
    "            # Upload markdown to S3 with the simplified name\n",
    "            s3_md_key = f\"{markdown_folder}{simplified_md_filename}\"\n",
    "            print(f\"Uploading markdown to S3: {s3_md_key}\")\n",
    "            s3.upload_file(simplified_md_path, bucket, s3_md_key)\n",
    "        else:\n",
    "            print(f\"Warning: Markdown file not created at expected path {md_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {pdf_filename}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Create an error markdown file\n",
    "        error_md_filename = f\"{year}.md\"\n",
    "        error_md_path = os.path.join(out_dir, error_md_filename)\n",
    "        \n",
    "        with open(error_md_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# Processing Error\\n\\nFailed to process {pdf_path}\\n\\nError: {str(e)}\")\n",
    "        \n",
    "        md_files.append(error_md_path)\n",
    "        \n",
    "        # Upload error markdown to S3\n",
    "        s3_md_key = f\"{markdown_folder}{error_md_filename}\"\n",
    "        print(f\"Uploading error markdown to S3: {s3_md_key}\")\n",
    "        s3.upload_file(error_md_path, bucket, s3_md_key)\n",
    "\n",
    "if not md_files:\n",
    "    print(\"No Markdown files generated.\")\n",
    "else:\n",
    "    print(f\"Successfully generated {len(md_files)} markdown files\")\n",
    "\n",
    "# Save the markdown file paths for later processing\n",
    "markdown_paths = {\n",
    "    \"md_files\": md_files,\n",
    "    \"timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "}\n",
    "\n",
    "with open(\"markdown_paths.json\", \"w\") as f:\n",
    "    json.dump(markdown_paths, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Complete\n",
    "\n",
    "The PDFs have been downloaded and converted to markdown files. These files have been uploaded to S3.\n",
    "\n",
    "To proceed with chunking and vector embedding, run the `embedding_and_chunking.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markdown Chunking and Vector Embedding \n",
    "Now that we have successfully converted the pdf files into markdowns ,we will be chunking it into json files for creating vectors and store it into pinecone index `crime-reports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking and creating vector embeddings...\n",
      "Loaded 24 markdown files from saved paths\n"
     ]
    }
   ],
   "source": [
    "print(\"Chunking and creating vector embeddings...\")\n",
    "\n",
    "# Check if we have markdown files\n",
    "try:\n",
    "    with open(\"markdown_paths.json\", \"r\") as f:\n",
    "        markdown_paths = json.load(f)\n",
    "        md_files = markdown_paths.get(\"md_files\", [])\n",
    "        \n",
    "    # Verify these files still exist\n",
    "    md_files = [f for f in md_files if os.path.exists(f)]\n",
    "    print(f\"Loaded {len(md_files)} markdown files from saved paths\")\n",
    "    \n",
    "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "    print(f\"Could not load markdown paths: {e}\")\n",
    "    \n",
    "    # If no markdown files were loaded, list them from S3\n",
    "    print(\"Downloading markdown files from S3...\")\n",
    "    resp = s3.list_objects_v2(Bucket=bucket, Prefix=markdown_folder)\n",
    "    md_files = []\n",
    "    \n",
    "    for obj in resp.get(\"Contents\", []):\n",
    "        if obj[\"Key\"].endswith(\".md\"):\n",
    "            filename = os.path.basename(obj[\"Key\"])\n",
    "            local_md = os.path.join(temp_dir, filename)\n",
    "            s3.download_file(bucket, obj[\"Key\"], local_md)\n",
    "            md_files.append(local_md)\n",
    "            print(f\"Downloaded: {filename}\")\n",
    "            \n",
    "    print(f\"Downloaded {len(md_files)} markdown files from S3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the markdown files are there or not !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following markdown files:\n",
      "1. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\1995.md\n",
      "2. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\1996.md\n",
      "3. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\1997.md\n",
      "4. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\1998.md\n",
      "5. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\1999.md\n",
      "6. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2000.md\n",
      "7. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2001.md\n",
      "8. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2002.md\n",
      "9. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2003.md\n",
      "10. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2004.md\n",
      "11. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2006.md\n",
      "12. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2007.md\n",
      "13. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2008.md\n",
      "14. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2009.md\n",
      "15. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2010.md\n",
      "16. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2011.md\n",
      "17. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2012.md\n",
      "18. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2013.md\n",
      "19. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2014.md\n",
      "20. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2015.md\n",
      "21. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2016.md\n",
      "22. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2017.md\n",
      "23. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2018.md\n",
      "24. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\mistral_output\\2019.md\n"
     ]
    }
   ],
   "source": [
    "if not md_files:\n",
    "    print(\"No markdown files found!\")\n",
    "else:\n",
    "    # Print all files for debugging\n",
    "    print(\"Found the following markdown files:\")\n",
    "    for i, file in enumerate(md_files):\n",
    "        print(f\"{i+1}. {file}\")\n",
    "        \n",
    "    # Initialize model and chunker\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    chunker = KamradtModifiedChunker(\n",
    "        avg_chunk_size=300,\n",
    "        min_chunk_size=50,\n",
    "        embedding_function=lambda texts: [model.encode(t).tolist() for t in texts]\n",
    "    )\n",
    "    \n",
    "    # Track overall statistics\n",
    "    total_processed = 0\n",
    "    total_failed = 0\n",
    "    processed_files = []\n",
    "    failed_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing markdown files and creating chunks...\n",
      "\n",
      "===== PROCESSING 1995.md =====\n",
      "Year detected: 1995\n",
      "Created 10 chunks\n",
      "Chunks will be stored at S3 path: chunks/1995_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\1995_chunks.json\n",
      "Uploading chunks to S3: chunks/1995_chunks.json\n",
      "S3 upload successful: chunks/1995_chunks.json\n",
      "✅ Successfully processed chunks for '1995.md'\n",
      "\n",
      "===== PROCESSING 1996.md =====\n",
      "Year detected: 1996\n",
      "Created 11 chunks\n",
      "Chunks will be stored at S3 path: chunks/1996_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\1996_chunks.json\n",
      "Uploading chunks to S3: chunks/1996_chunks.json\n",
      "S3 upload successful: chunks/1996_chunks.json\n",
      "✅ Successfully processed chunks for '1996.md'\n",
      "\n",
      "===== PROCESSING 1997.md =====\n",
      "Year detected: 1997\n",
      "Created 13 chunks\n",
      "Chunks will be stored at S3 path: chunks/1997_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\1997_chunks.json\n",
      "Uploading chunks to S3: chunks/1997_chunks.json\n",
      "S3 upload successful: chunks/1997_chunks.json\n",
      "✅ Successfully processed chunks for '1997.md'\n",
      "\n",
      "===== PROCESSING 1998.md =====\n",
      "Year detected: 1998\n",
      "Created 10 chunks\n",
      "Chunks will be stored at S3 path: chunks/1998_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\1998_chunks.json\n",
      "Uploading chunks to S3: chunks/1998_chunks.json\n",
      "S3 upload successful: chunks/1998_chunks.json\n",
      "✅ Successfully processed chunks for '1998.md'\n",
      "\n",
      "===== PROCESSING 1999.md =====\n",
      "Year detected: 1999\n",
      "Created 10 chunks\n",
      "Chunks will be stored at S3 path: chunks/1999_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\1999_chunks.json\n",
      "Uploading chunks to S3: chunks/1999_chunks.json\n",
      "S3 upload successful: chunks/1999_chunks.json\n",
      "✅ Successfully processed chunks for '1999.md'\n",
      "\n",
      "===== PROCESSING 2000.md =====\n",
      "Year detected: 2000\n",
      "Created 10 chunks\n",
      "Chunks will be stored at S3 path: chunks/2000_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2000_chunks.json\n",
      "Uploading chunks to S3: chunks/2000_chunks.json\n",
      "S3 upload successful: chunks/2000_chunks.json\n",
      "✅ Successfully processed chunks for '2000.md'\n",
      "\n",
      "===== PROCESSING 2001.md =====\n",
      "Year detected: 2001\n",
      "Created 13 chunks\n",
      "Chunks will be stored at S3 path: chunks/2001_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2001_chunks.json\n",
      "Uploading chunks to S3: chunks/2001_chunks.json\n",
      "S3 upload successful: chunks/2001_chunks.json\n",
      "✅ Successfully processed chunks for '2001.md'\n",
      "\n",
      "===== PROCESSING 2002.md =====\n",
      "Year detected: 2002\n",
      "Created 9 chunks\n",
      "Chunks will be stored at S3 path: chunks/2002_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2002_chunks.json\n",
      "Uploading chunks to S3: chunks/2002_chunks.json\n",
      "S3 upload successful: chunks/2002_chunks.json\n",
      "✅ Successfully processed chunks for '2002.md'\n",
      "\n",
      "===== PROCESSING 2003.md =====\n",
      "Year detected: 2003\n",
      "Created 1 chunks\n",
      "Chunks will be stored at S3 path: chunks/2003_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2003_chunks.json\n",
      "Uploading chunks to S3: chunks/2003_chunks.json\n",
      "S3 upload successful: chunks/2003_chunks.json\n",
      "✅ Successfully processed chunks for '2003.md'\n",
      "\n",
      "===== PROCESSING 2004.md =====\n",
      "Year detected: 2004\n",
      "Created 658 chunks\n",
      "Chunks will be stored at S3 path: chunks/2004_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2004_chunks.json\n",
      "Uploading chunks to S3: chunks/2004_chunks.json\n",
      "S3 upload successful: chunks/2004_chunks.json\n",
      "✅ Successfully processed chunks for '2004.md'\n",
      "\n",
      "===== PROCESSING 2006.md =====\n",
      "Year detected: 2006\n",
      "Created 4 chunks\n",
      "Chunks will be stored at S3 path: chunks/2006_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2006_chunks.json\n",
      "Uploading chunks to S3: chunks/2006_chunks.json\n",
      "S3 upload successful: chunks/2006_chunks.json\n",
      "✅ Successfully processed chunks for '2006.md'\n",
      "\n",
      "===== PROCESSING 2007.md =====\n",
      "Year detected: 2007\n",
      "Created 3 chunks\n",
      "Chunks will be stored at S3 path: chunks/2007_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2007_chunks.json\n",
      "Uploading chunks to S3: chunks/2007_chunks.json\n",
      "S3 upload successful: chunks/2007_chunks.json\n",
      "✅ Successfully processed chunks for '2007.md'\n",
      "\n",
      "===== PROCESSING 2008.md =====\n",
      "Year detected: 2008\n",
      "Created 3 chunks\n",
      "Chunks will be stored at S3 path: chunks/2008_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2008_chunks.json\n",
      "Uploading chunks to S3: chunks/2008_chunks.json\n",
      "S3 upload successful: chunks/2008_chunks.json\n",
      "✅ Successfully processed chunks for '2008.md'\n",
      "\n",
      "===== PROCESSING 2009.md =====\n",
      "Year detected: 2009\n",
      "Created 4 chunks\n",
      "Chunks will be stored at S3 path: chunks/2009_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2009_chunks.json\n",
      "Uploading chunks to S3: chunks/2009_chunks.json\n",
      "S3 upload successful: chunks/2009_chunks.json\n",
      "✅ Successfully processed chunks for '2009.md'\n",
      "\n",
      "===== PROCESSING 2010.md =====\n",
      "Year detected: 2010\n",
      "Created 3 chunks\n",
      "Chunks will be stored at S3 path: chunks/2010_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2010_chunks.json\n",
      "Uploading chunks to S3: chunks/2010_chunks.json\n",
      "S3 upload successful: chunks/2010_chunks.json\n",
      "✅ Successfully processed chunks for '2010.md'\n",
      "\n",
      "===== PROCESSING 2011.md =====\n",
      "Year detected: 2011\n",
      "Created 4 chunks\n",
      "Chunks will be stored at S3 path: chunks/2011_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2011_chunks.json\n",
      "Uploading chunks to S3: chunks/2011_chunks.json\n",
      "S3 upload successful: chunks/2011_chunks.json\n",
      "✅ Successfully processed chunks for '2011.md'\n",
      "\n",
      "===== PROCESSING 2012.md =====\n",
      "Year detected: 2012\n",
      "Created 3 chunks\n",
      "Chunks will be stored at S3 path: chunks/2012_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2012_chunks.json\n",
      "Uploading chunks to S3: chunks/2012_chunks.json\n",
      "S3 upload successful: chunks/2012_chunks.json\n",
      "✅ Successfully processed chunks for '2012.md'\n",
      "\n",
      "===== PROCESSING 2013.md =====\n",
      "Year detected: 2013\n",
      "Created 4 chunks\n",
      "Chunks will be stored at S3 path: chunks/2013_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2013_chunks.json\n",
      "Uploading chunks to S3: chunks/2013_chunks.json\n",
      "S3 upload successful: chunks/2013_chunks.json\n",
      "✅ Successfully processed chunks for '2013.md'\n",
      "\n",
      "===== PROCESSING 2014.md =====\n",
      "Year detected: 2014\n",
      "Created 4 chunks\n",
      "Chunks will be stored at S3 path: chunks/2014_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2014_chunks.json\n",
      "Uploading chunks to S3: chunks/2014_chunks.json\n",
      "S3 upload successful: chunks/2014_chunks.json\n",
      "✅ Successfully processed chunks for '2014.md'\n",
      "\n",
      "===== PROCESSING 2015.md =====\n",
      "Year detected: 2015\n",
      "Created 1 chunks\n",
      "Chunks will be stored at S3 path: chunks/2015_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2015_chunks.json\n",
      "Uploading chunks to S3: chunks/2015_chunks.json\n",
      "S3 upload successful: chunks/2015_chunks.json\n",
      "✅ Successfully processed chunks for '2015.md'\n",
      "\n",
      "===== PROCESSING 2016.md =====\n",
      "Year detected: 2016\n",
      "Created 4 chunks\n",
      "Chunks will be stored at S3 path: chunks/2016_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2016_chunks.json\n",
      "Uploading chunks to S3: chunks/2016_chunks.json\n",
      "S3 upload successful: chunks/2016_chunks.json\n",
      "✅ Successfully processed chunks for '2016.md'\n",
      "\n",
      "===== PROCESSING 2017.md =====\n",
      "Year detected: 2017\n",
      "Created 4 chunks\n",
      "Chunks will be stored at S3 path: chunks/2017_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2017_chunks.json\n",
      "Uploading chunks to S3: chunks/2017_chunks.json\n",
      "S3 upload successful: chunks/2017_chunks.json\n",
      "✅ Successfully processed chunks for '2017.md'\n",
      "\n",
      "===== PROCESSING 2018.md =====\n",
      "Year detected: 2018\n",
      "Created 3 chunks\n",
      "Chunks will be stored at S3 path: chunks/2018_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2018_chunks.json\n",
      "Uploading chunks to S3: chunks/2018_chunks.json\n",
      "S3 upload successful: chunks/2018_chunks.json\n",
      "✅ Successfully processed chunks for '2018.md'\n",
      "\n",
      "===== PROCESSING 2019.md =====\n",
      "Year detected: 2019\n",
      "Created 3 chunks\n",
      "Chunks will be stored at S3 path: chunks/2019_chunks.json\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_9tmj6okr\\2019_chunks.json\n",
      "Uploading chunks to S3: chunks/2019_chunks.json\n",
      "S3 upload successful: chunks/2019_chunks.json\n",
      "✅ Successfully processed chunks for '2019.md'\n",
      "\n",
      "===== CHUNK PROCESSING SUMMARY =====\n",
      "Total files processed successfully: 24\n",
      "Total files failed: 0\n",
      "\n",
      "All markdown files chunked and uploaded to S3!\n"
     ]
    }
   ],
   "source": [
    "# --- Create and upload chunks to S3 ---\n",
    "import json, uuid, os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "print(\"Processing markdown files and creating chunks...\")\n",
    "\n",
    "# Track overall statistics\n",
    "total_processed = 0\n",
    "total_failed = 0\n",
    "processed_files = []\n",
    "failed_files = []\n",
    "all_chunk_data = {}  # Store chunk data for later vector embedding\n",
    "\n",
    "# Process each markdown file for chunking\n",
    "for md_file in md_files:\n",
    "    try:\n",
    "        # Validate file exists\n",
    "        if not os.path.exists(md_file):\n",
    "            print(f\"WARNING: File not found: {md_file}\")\n",
    "            total_failed += 1\n",
    "            failed_files.append(md_file)\n",
    "            continue\n",
    "            \n",
    "        with open(md_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Get the filename to extract year\n",
    "        md_filename = os.path.basename(md_file)\n",
    "        print(f\"\\n===== PROCESSING {md_filename} =====\")\n",
    "        \n",
    "        # Extract year from the filename (should be the filename itself without extension)\n",
    "        year = os.path.splitext(md_filename)[0]\n",
    "        if not re.match(r'^(19|20)\\d{2}$', year):\n",
    "            # If filename isn't just a year, try to extract year\n",
    "            year = extract_year_from_filename(md_filename)\n",
    "            \n",
    "            # If still no year, try from content\n",
    "            if not year:\n",
    "                year = extract_year_from_content(content)\n",
    "                \n",
    "        print(f\"Year detected: {year}\")\n",
    "                \n",
    "        # Process chunks\n",
    "        chunks = chunker.split_text(content)\n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Create simplified chunk filename: year_chunks.json\n",
    "        chunks_json_filename = f\"{year}_chunks.json\"\n",
    "        chunks_json_path = os.path.join(temp_dir, chunks_json_filename)\n",
    "        \n",
    "        # S3 path for storing and referencing chunks\n",
    "        s3_chunks_key = f\"{chunks_folder}{chunks_json_filename}\"\n",
    "        print(f\"Chunks will be stored at S3 path: {s3_chunks_key}\")\n",
    "        \n",
    "        # Check if this chunks file already exists in S3\n",
    "        existing_chunks = []\n",
    "        try:\n",
    "            response = s3.get_object(Bucket=bucket, Key=s3_chunks_key)\n",
    "            existing_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            existing_chunks = existing_data.get(\"chunks\", [])\n",
    "            print(f\"Found existing chunks file with {len(existing_chunks)} chunks\")\n",
    "        except Exception as e:\n",
    "            if \"NoSuchKey\" not in str(e):  # Only log if it's not just a missing file\n",
    "                print(f\"No existing chunks file found: {str(e)}\")\n",
    "        \n",
    "        # Combine with new chunks if there were existing ones\n",
    "        all_chunks = existing_chunks + chunks\n",
    "        \n",
    "        chunks_data = {\n",
    "            \"source_file\": md_filename,\n",
    "            \"chunks\": all_chunks,\n",
    "            \"chunk_count\": len(all_chunks),\n",
    "            \"year\": year,\n",
    "            \"processed_date\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"document_id\": year,  # Use year as document ID\n",
    "            \"s3_location\": s3_chunks_key\n",
    "        }\n",
    "        \n",
    "        # Save locally first\n",
    "        with open(chunks_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        # Verify the file was created locally\n",
    "        if not os.path.exists(chunks_json_path):\n",
    "            print(f\"ERROR: Failed to create local chunks file: {chunks_json_path}\")\n",
    "        else:\n",
    "            print(f\"Successfully created local chunks file: {chunks_json_path}\")\n",
    "            \n",
    "        # Upload chunks JSON to S3\n",
    "        print(f\"Uploading chunks to S3: {s3_chunks_key}\")\n",
    "        s3.upload_file(chunks_json_path, bucket, s3_chunks_key)\n",
    "        \n",
    "        # Verify the S3 upload\n",
    "        try:\n",
    "            s3.head_object(Bucket=bucket, Key=s3_chunks_key)\n",
    "            print(f\"S3 upload successful: {s3_chunks_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: S3 upload verification failed: {str(e)}\")\n",
    "            \n",
    "        # Store data for later vector embedding\n",
    "        all_chunk_data[md_file] = {\n",
    "            \"year\": year,\n",
    "            \"chunks\": chunks,\n",
    "            \"existing_chunks_count\": len(existing_chunks),\n",
    "            \"s3_chunks_key\": s3_chunks_key,\n",
    "            \"md_filename\": md_filename\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Successfully processed chunks for '{md_filename}'\")\n",
    "        total_processed += 1\n",
    "        processed_files.append(md_file)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR processing markdown file {md_file}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        total_failed += 1\n",
    "        failed_files.append(md_file)\n",
    "\n",
    "# Print summary of chunk processing\n",
    "print(\"\\n===== CHUNK PROCESSING SUMMARY =====\")\n",
    "print(f\"Total files processed successfully: {total_processed}\")\n",
    "print(f\"Total files failed: {total_failed}\")\n",
    "\n",
    "if failed_files:\n",
    "    print(\"\\nFailed files:\")\n",
    "    for f in failed_files:\n",
    "        print(f\"- {f}\")\n",
    "\n",
    "print(\"\\nAll markdown files chunked and uploaded to S3!\")\n",
    "\n",
    "# Save chunk data for the next cell\n",
    "with open(\"chunk_data.json\", \"w\") as f:\n",
    "    # Convert to serializable format\n",
    "    serializable_data = {}\n",
    "    for md_file, data in all_chunk_data.items():\n",
    "        serializable_data[md_file] = {\n",
    "            \"year\": data[\"year\"],\n",
    "            \"chunks_count\": len(data[\"chunks\"]),\n",
    "            \"existing_chunks_count\": data[\"existing_chunks_count\"],\n",
    "            \"s3_chunks_key\": data[\"s3_chunks_key\"],\n",
    "            \"md_filename\": data[\"md_filename\"]\n",
    "        }\n",
    "    json.dump(serializable_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embedding and Pinecone Upload (Separate Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and uploading to Pinecone...\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 1995.md =====\n",
      "Year for embedding: 1995\n",
      "Creating embeddings for 10 chunks\n",
      "Upserting 10 vectors to Pinecone namespace: 1995\n",
      "✅ Successfully embedded 10 chunks for '1995.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 1996.md =====\n",
      "Year for embedding: 1996\n",
      "Creating embeddings for 11 chunks\n",
      "Upserting 11 vectors to Pinecone namespace: 1996\n",
      "✅ Successfully embedded 11 chunks for '1996.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 1997.md =====\n",
      "Year for embedding: 1997\n",
      "Creating embeddings for 13 chunks\n",
      "Upserting 13 vectors to Pinecone namespace: 1997\n",
      "✅ Successfully embedded 13 chunks for '1997.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 1998.md =====\n",
      "Year for embedding: 1998\n",
      "Creating embeddings for 10 chunks\n",
      "Upserting 10 vectors to Pinecone namespace: 1998\n",
      "✅ Successfully embedded 10 chunks for '1998.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 1999.md =====\n",
      "Year for embedding: 1999\n",
      "Creating embeddings for 10 chunks\n",
      "Upserting 10 vectors to Pinecone namespace: 1999\n",
      "✅ Successfully embedded 10 chunks for '1999.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2000.md =====\n",
      "Year for embedding: 2000\n",
      "Creating embeddings for 10 chunks\n",
      "Upserting 10 vectors to Pinecone namespace: 2000\n",
      "✅ Successfully embedded 10 chunks for '2000.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2001.md =====\n",
      "Year for embedding: 2001\n",
      "Creating embeddings for 13 chunks\n",
      "Upserting 13 vectors to Pinecone namespace: 2001\n",
      "✅ Successfully embedded 13 chunks for '2001.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2002.md =====\n",
      "Year for embedding: 2002\n",
      "Creating embeddings for 9 chunks\n",
      "Upserting 9 vectors to Pinecone namespace: 2002\n",
      "✅ Successfully embedded 9 chunks for '2002.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2003.md =====\n",
      "Year for embedding: 2003\n",
      "Creating embeddings for 1 chunks\n",
      "Upserting 1 vectors to Pinecone namespace: 2003\n",
      "✅ Successfully embedded 1 chunks for '2003.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2004.md =====\n",
      "Year for embedding: 2004\n",
      "Creating embeddings for 658 chunks\n",
      "Upserting 658 vectors to Pinecone namespace: 2004\n",
      "✅ Successfully embedded 658 chunks for '2004.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2006.md =====\n",
      "Year for embedding: 2006\n",
      "Creating embeddings for 4 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2006\n",
      "✅ Successfully embedded 4 chunks for '2006.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2007.md =====\n",
      "Year for embedding: 2007\n",
      "Creating embeddings for 3 chunks\n",
      "Upserting 3 vectors to Pinecone namespace: 2007\n",
      "✅ Successfully embedded 3 chunks for '2007.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2008.md =====\n",
      "Year for embedding: 2008\n",
      "Creating embeddings for 3 chunks\n",
      "Upserting 3 vectors to Pinecone namespace: 2008\n",
      "✅ Successfully embedded 3 chunks for '2008.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2009.md =====\n",
      "Year for embedding: 2009\n",
      "Creating embeddings for 4 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2009\n",
      "✅ Successfully embedded 4 chunks for '2009.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2010.md =====\n",
      "Year for embedding: 2010\n",
      "Creating embeddings for 3 chunks\n",
      "Upserting 3 vectors to Pinecone namespace: 2010\n",
      "✅ Successfully embedded 3 chunks for '2010.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2011.md =====\n",
      "Year for embedding: 2011\n",
      "Creating embeddings for 4 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2011\n",
      "✅ Successfully embedded 4 chunks for '2011.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2012.md =====\n",
      "Year for embedding: 2012\n",
      "Creating embeddings for 3 chunks\n",
      "Upserting 3 vectors to Pinecone namespace: 2012\n",
      "✅ Successfully embedded 3 chunks for '2012.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2013.md =====\n",
      "Year for embedding: 2013\n",
      "Creating embeddings for 4 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2013\n",
      "✅ Successfully embedded 4 chunks for '2013.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2014.md =====\n",
      "Year for embedding: 2014\n",
      "Creating embeddings for 4 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2014\n",
      "✅ Successfully embedded 4 chunks for '2014.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2015.md =====\n",
      "Year for embedding: 2015\n",
      "Creating embeddings for 1 chunks\n",
      "Upserting 1 vectors to Pinecone namespace: 2015\n",
      "✅ Successfully embedded 1 chunks for '2015.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2016.md =====\n",
      "Year for embedding: 2016\n",
      "Creating embeddings for 4 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2016\n",
      "✅ Successfully embedded 4 chunks for '2016.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2017.md =====\n",
      "Year for embedding: 2017\n",
      "Creating embeddings for 4 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2017\n",
      "✅ Successfully embedded 4 chunks for '2017.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2018.md =====\n",
      "Year for embedding: 2018\n",
      "Creating embeddings for 3 chunks\n",
      "Upserting 3 vectors to Pinecone namespace: 2018\n",
      "✅ Successfully embedded 3 chunks for '2018.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2019.md =====\n",
      "Year for embedding: 2019\n",
      "Creating embeddings for 3 chunks\n",
      "Upserting 3 vectors to Pinecone namespace: 2019\n",
      "✅ Successfully embedded 3 chunks for '2019.md'\n",
      "\n",
      "===== EMBEDDING SUMMARY =====\n",
      "Total files embedded: 24\n",
      "Total vectors created: 792\n",
      "Embedding errors: 0\n",
      "\n",
      "Vector embedding complete!\n"
     ]
    }
   ],
   "source": [
    "# --- Vector Embedding and Pinecone Upload ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json, uuid\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "\n",
    "print(\"Creating embeddings and uploading to Pinecone...\")\n",
    "\n",
    "# Initialize model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Make sure we have the index initialized\n",
    "if 'index' not in locals():\n",
    "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "    index = pc.Index(\"crime-reports\")\n",
    "\n",
    "# Track embedding progress\n",
    "total_vectors = 0\n",
    "total_files_embedded = 0\n",
    "embedding_errors = 0\n",
    "\n",
    "# Process each markdown file\n",
    "for md_file in md_files:\n",
    "    try:\n",
    "        # Get the filename and content\n",
    "        md_filename = os.path.basename(md_file)\n",
    "        print(f\"\\n===== CREATING EMBEDDINGS FOR {md_filename} =====\")\n",
    "        \n",
    "        with open(md_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Get year\n",
    "        year = os.path.splitext(md_filename)[0]\n",
    "        if not re.match(r'^(19|20)\\d{2}$', year):\n",
    "            year = extract_year_from_filename(md_filename)\n",
    "            if not year:\n",
    "                year = extract_year_from_content(content)\n",
    "                \n",
    "        print(f\"Year for embedding: {year}\")\n",
    "        \n",
    "        # Recreate chunks (we don't load from chunk_data.json to avoid losing chunk text)\n",
    "        chunks = chunker.split_text(content)\n",
    "        print(f\"Creating embeddings for {len(chunks)} chunks\")\n",
    "        \n",
    "        # S3 path for retrieving chunks\n",
    "        chunks_json_filename = f\"{year}_chunks.json\"\n",
    "        s3_chunks_key = f\"{chunks_folder}{chunks_json_filename}\"\n",
    "        \n",
    "        # Create vector embeddings\n",
    "        vectors = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Create embedding\n",
    "            emb = model.encode(chunk).tolist()\n",
    "            \n",
    "            # Create metadata for retrieval\n",
    "            metadata = {\n",
    "                \"file\": md_filename,\n",
    "                \"document_id\": year,  # Use year as document ID\n",
    "                \"chunk_index\": i,\n",
    "                \"text_preview\": chunk[:100] + (\"...\" if len(chunk) > 100 else \"\"),\n",
    "                \"chunks_s3_path\": s3_chunks_key,\n",
    "                \"s3_bucket\": bucket,\n",
    "                \"year\": year,\n",
    "                \"chunk_length\": len(chunk),\n",
    "                \"time_processed\": datetime.now(timezone.utc).isoformat()\n",
    "            }\n",
    "                \n",
    "            vectors.append((\n",
    "                f\"{year}_{i}_{uuid.uuid4()}\",  # ID includes year and chunk index\n",
    "                emb,\n",
    "                metadata\n",
    "            ))\n",
    "            \n",
    "        # Upsert in batches\n",
    "        print(f\"Upserting {len(vectors)} vectors to Pinecone namespace: {year}\")\n",
    "        \n",
    "        for i in range(0, len(vectors), 100):\n",
    "            batch = vectors[i:i+100]\n",
    "            # Use year as namespace for easy filtering\n",
    "            index.upsert(vectors=batch, namespace=year)\n",
    "        \n",
    "        total_vectors += len(vectors)\n",
    "        total_files_embedded += 1\n",
    "        print(f\"✅ Successfully embedded {len(vectors)} chunks for '{md_filename}'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR creating embeddings for {md_file}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        embedding_errors += 1\n",
    "\n",
    "# Print embedding summary\n",
    "print(\"\\n===== EMBEDDING SUMMARY =====\")\n",
    "print(f\"Total files embedded: {total_files_embedded}\")\n",
    "print(f\"Total vectors created: {total_vectors}\")\n",
    "print(f\"Embedding errors: {embedding_errors}\")\n",
    "print(\"\\nVector embedding complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check if Pinecone index is created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Pinecone connection...\n",
      "Found Pinecone API key: pcsk...\n",
      "\n",
      "Current Pinecone indexes:\n",
      "1. embedding-cosine\n",
      "2. crime-records\n",
      "3. gpt-4o-research-agent\n",
      "4. pinecone-embeddings\n",
      "5. nvidia-reports\n",
      "\n",
      "Success: Index 'crime-records' exists in Pinecone.\n",
      "Connected to index 'crime-records'\n"
     ]
    }
   ],
   "source": [
    "# --- Comprehensive Pinecone Setup ---\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "print(\"Setting up Pinecone connection...\")\n",
    "\n",
    "# Load environment variables if not already loaded\n",
    "if \"PINECONE_API_KEY\" not in os.environ:\n",
    "    load_dotenv()\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not pinecone_api_key:\n",
    "    print(\"ERROR: PINECONE_API_KEY not found in environment variables\")\n",
    "    print(\"Please check your .env file or set it manually.\")\n",
    "else:\n",
    "    print(f\"Found Pinecone API key: {pinecone_api_key[:4]}...\")  # Show first 4 chars only for security\n",
    "\n",
    "# Initialize Pinecone client\n",
    "try:\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    \n",
    "    # List all available indexes\n",
    "    all_indexes = pc.list_indexes()\n",
    "    print(\"\\nCurrent Pinecone indexes:\")\n",
    "    if not all_indexes.names():\n",
    "        print(\"No indexes found in this Pinecone account.\")\n",
    "    else:\n",
    "        for i, name in enumerate(all_indexes.names()):\n",
    "            print(f\"{i+1}. {name}\")\n",
    "    \n",
    "    # Set our target index name\n",
    "    index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "    \n",
    "    # Check if the index exists\n",
    "    if index_name in all_indexes.names():\n",
    "        print(f\"\\nSuccess: Index '{index_name}' exists in Pinecone.\")\n",
    "        index = pc.Index(index_name)\n",
    "        print(f\"Connected to index '{index_name}'\")\n",
    "    else:\n",
    "        print(f\"\\nIndex '{index_name}' does not exist in Pinecone. Creating...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR connecting to Pinecone: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'1995': {'vector_count': 10},\n",
      "                '1996': {'vector_count': 11},\n",
      "                '1997': {'vector_count': 13},\n",
      "                '1998': {'vector_count': 10},\n",
      "                '1999': {'vector_count': 10},\n",
      "                '2000': {'vector_count': 10},\n",
      "                '2001': {'vector_count': 13},\n",
      "                '2002': {'vector_count': 9},\n",
      "                '2003': {'vector_count': 1},\n",
      "                '2004': {'vector_count': 658},\n",
      "                '2006': {'vector_count': 4},\n",
      "                '2007': {'vector_count': 3},\n",
      "                '2008': {'vector_count': 3},\n",
      "                '2009': {'vector_count': 4},\n",
      "                '2010': {'vector_count': 3},\n",
      "                '2011': {'vector_count': 4},\n",
      "                '2012': {'vector_count': 3},\n",
      "                '2013': {'vector_count': 4},\n",
      "                '2014': {'vector_count': 4},\n",
      "                '2015': {'vector_count': 1},\n",
      "                '2016': {'vector_count': 4},\n",
      "                '2017': {'vector_count': 4},\n",
      "                '2018': {'vector_count': 3},\n",
      "                '2019': {'vector_count': 3}},\n",
      " 'total_vector_count': 792,\n",
      " 'vector_type': 'dense'}\n",
      "Total vectors in index: 792\n"
     ]
    }
   ],
   "source": [
    "stats = index.describe_index_stats()\n",
    "print(f\"\\nIndex stats: {stats}\")\n",
    "print(f\"Total vectors in index: {stats['total_vector_count']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-pipeline-airflow-3n8gKQVQ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
