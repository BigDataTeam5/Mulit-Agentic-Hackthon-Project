{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Data PDF Processing Pipeline\n",
    "\n",
    "This notebook processes FBI crime statistic PDFs through the following steps:\n",
    "1. Download PDFs from S3\n",
    "2. Convert PDFs to markdown using Mistral OCR\n",
    "3. Upload generated markdown to S3\n",
    "4. Extract year information\n",
    "5. Create chunks for embedding\n",
    "6. Upload chunks to S3\n",
    "7. Create vector embeddings in Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shush\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\rag-pipeline-airflow-3n8gKQVQ-py3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded API Key: R9wv...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, uuid, shutil, tempfile, re, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Custom modules - update these paths if needed:\n",
    "from mistralparsing_userpdf import process_pdf as mistral_process_pdf\n",
    "from utils.chunking import KamradtModifiedChunker\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Year Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_from_filename(filename):\n",
    "    \"\"\"Extract a 4-digit year from a filename.\"\"\"\n",
    "    # Look for 4 consecutive digits that likely represent a year (between 1900 and 2099)\n",
    "    match = re.search(r'(?:19|20)\\d{2}', filename)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None  # Return None if no year found\n",
    "\n",
    "def extract_year_from_content(content):\n",
    "    \"\"\"Extract year from markdown content based on common patterns.\"\"\"\n",
    "    # Pattern for \"FBI Releases YYYY Crime Statistics\" or similar\n",
    "    title_match = re.search(r'FBI Releases (\\d{4}) Crime Statistics', content)\n",
    "    if title_match:\n",
    "        return title_match.group(1)\n",
    "        \n",
    "    # Look for years in the text that are likely report years\n",
    "    year_matches = re.finditer(r'(?:in|for|during|of)(?: the)? (?:year )?(\\d{4})', content.lower())\n",
    "    for match in year_matches:\n",
    "        year = match.group(1)\n",
    "        # Validate year is between 1900 and current year\n",
    "        if 1900 <= int(year) <= datetime.now().year:\n",
    "            return year\n",
    "            \n",
    "    # Last resort: just find any 4-digit number that looks like a year\n",
    "    general_year = re.search(r'\\b((?:19|20)\\d{2})\\b', content)\n",
    "    if general_year:\n",
    "        return general_year.group(1)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 and Pinecone Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- S3 Setup ---\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=os.getenv(\"AWS_SERVER_PUBLIC_KEY\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SERVER_SECRET_KEY\")\n",
    ")\n",
    "bucket = os.getenv(\"BUCKET_NAME\")\n",
    "input_folder = \"crime records summary/\"  # The folder with PDFs in S3\n",
    "markdown_folder = \"processed_markdown/\"  # Where to store processed markdown files\n",
    "chunks_folder = \"chunks/\"  # Where to store chunks\n",
    "\n",
    "# --- Pinecone Setup ---\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "# Delete index if it exists\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "# Create new index with ServerlessSpec\n",
    "pc.create_index(\n",
    "    name=index_name, \n",
    "    dimension=384, \n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"  # Adjust region as needed\n",
    "    )\n",
    ")\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# --- Temp Dir ---\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"pdf_downloads_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download PDFs from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PDFs from S3...\n",
      "Downloaded: 1995Summary.pdf\n",
      "Downloaded: 1996Summary.pdf\n",
      "Downloaded: 1997Summary.pdf\n",
      "Downloaded: 1998Summary.pdf\n",
      "Downloaded: 1999Summary.pdf\n",
      "Downloaded: 2000Summary.pdf\n",
      "Downloaded: 2001Summary.pdf\n",
      "Downloaded: 2002Summary.pdf\n",
      "Downloaded: 2003Summary.pdf\n",
      "Downloaded: 2004Summary.pdf\n",
      "Downloaded: 2006Summary.pdf\n",
      "Downloaded: 2007 CIUS Summary.pdf\n",
      "Downloaded: 2008 CIUS Summary.pdf\n",
      "Downloaded: 2009Summary.pdf\n",
      "Downloaded: 2010 CIUS Summary.pdf\n",
      "Downloaded: 2011Summary.pdf\n",
      "Downloaded: 2012 CIUS Summary.pdf\n",
      "Downloaded: 2013 CIUS Summary _final.pdf\n",
      "Downloaded: 2014 CIUS Summary_final.pdf\n",
      "Downloaded: 2015 CIUS Summary_final.pdf\n",
      "Downloaded: 2016 CIUS Summary.pdf\n",
      "Downloaded: 2017 CIUS Summary.pdf\n",
      "Downloaded: 2018 CIUS Summary.pdf\n",
      "Downloaded: 2019 CIUS Summary.pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading PDFs from S3...\")\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=input_folder)\n",
    "pdf_paths = []\n",
    "original_filenames = {}  # To keep track of original filenames for year extraction\n",
    "\n",
    "for obj in resp.get(\"Contents\", []):\n",
    "    if obj[\"Key\"].endswith(\".pdf\"):\n",
    "        filename = os.path.basename(obj[\"Key\"])\n",
    "        local_pdf = os.path.join(temp_dir, filename)\n",
    "        s3.download_file(bucket, obj[\"Key\"], local_pdf)\n",
    "        pdf_paths.append(local_pdf)\n",
    "        original_filenames[local_pdf] = filename  \n",
    "        print(f\"Downloaded: {filename}\")\n",
    "\n",
    "if not pdf_paths:\n",
    "    print(\"No PDFs found in the S3 folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert PDFs to Markdown and  Save Markdown File Paths for Later Processing\n",
    "\n",
    "We'll save the markdown file paths to a JSON file so we can process them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting PDFs to Markdown...\n",
      "Processing 1995Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\1995Summary.pdf\n",
      "Processing 1995Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 1995.md\n",
      "Uploading markdown to S3: processed_markdown/1995.md\n",
      "Processing 1996Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\1996Summary.pdf\n",
      "Processing 1996Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 1996.md\n",
      "Uploading markdown to S3: processed_markdown/1996.md\n",
      "Processing 1997Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\1997Summary.pdf\n",
      "Processing 1997Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 1997.md\n",
      "Uploading markdown to S3: processed_markdown/1997.md\n",
      "Processing 1998Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\1998Summary.pdf\n",
      "Processing 1998Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 1998.md\n",
      "Uploading markdown to S3: processed_markdown/1998.md\n",
      "Processing 1999Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\1999Summary.pdf\n",
      "Processing 1999Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 1999.md\n",
      "Uploading markdown to S3: processed_markdown/1999.md\n",
      "Processing 2000Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2000Summary.pdf\n",
      "Processing 2000Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2000.md\n",
      "Uploading markdown to S3: processed_markdown/2000.md\n",
      "Processing 2001Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2001Summary.pdf\n",
      "Processing 2001Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2001.md\n",
      "Uploading markdown to S3: processed_markdown/2001.md\n",
      "Processing 2002Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2002Summary.pdf\n",
      "Processing 2002Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2002.md\n",
      "Uploading markdown to S3: processed_markdown/2002.md\n",
      "Processing 2003Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2003Summary.pdf\n",
      "Processing 2003Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2003.md\n",
      "Uploading markdown to S3: processed_markdown/2003.md\n",
      "Processing 2004Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2004Summary.pdf\n",
      "Processing 2004Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2004.md\n",
      "Uploading markdown to S3: processed_markdown/2004.md\n",
      "Processing 2006Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2006Summary.pdf\n",
      "Processing 2006Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2006.md\n",
      "Uploading markdown to S3: processed_markdown/2006.md\n",
      "Processing 2007 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2007 CIUS Summary.pdf\n",
      "Processing 2007 CIUS Summary.pdf ...\n",
      "Error processing PDF: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Error markdown saved to C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2007.md\n",
      "Uploading markdown to S3: processed_markdown/2007.md\n",
      "Processing 2008 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2008 CIUS Summary.pdf\n",
      "Processing 2008 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2008.md\n",
      "Uploading markdown to S3: processed_markdown/2008.md\n",
      "Processing 2009Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2009Summary.pdf\n",
      "Processing 2009Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2009.md\n",
      "Uploading markdown to S3: processed_markdown/2009.md\n",
      "Processing 2010 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2010 CIUS Summary.pdf\n",
      "Processing 2010 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2010.md\n",
      "Uploading markdown to S3: processed_markdown/2010.md\n",
      "Processing 2011Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2011Summary.pdf\n",
      "Processing 2011Summary.pdf ...\n",
      "Error processing PDF: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Error markdown saved to C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2011.md\n",
      "Uploading markdown to S3: processed_markdown/2011.md\n",
      "Processing 2012 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2012 CIUS Summary.pdf\n",
      "Processing 2012 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2012.md\n",
      "Uploading markdown to S3: processed_markdown/2012.md\n",
      "Processing 2013 CIUS Summary _final.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2013 CIUS Summary _final.pdf\n",
      "Processing 2013 CIUS Summary _final.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2013.md\n",
      "Uploading markdown to S3: processed_markdown/2013.md\n",
      "Processing 2014 CIUS Summary_final.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2014 CIUS Summary_final.pdf\n",
      "Processing 2014 CIUS Summary_final.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2014.md\n",
      "Uploading markdown to S3: processed_markdown/2014.md\n",
      "Processing 2015 CIUS Summary_final.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2015 CIUS Summary_final.pdf\n",
      "Processing 2015 CIUS Summary_final.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2015.md\n",
      "Uploading markdown to S3: processed_markdown/2015.md\n",
      "Processing 2016 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2016 CIUS Summary.pdf\n",
      "Processing 2016 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2016.md\n",
      "Uploading markdown to S3: processed_markdown/2016.md\n",
      "Processing 2017 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2017 CIUS Summary.pdf\n",
      "Processing 2017 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2017.md\n",
      "Uploading markdown to S3: processed_markdown/2017.md\n",
      "Processing 2018 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2018 CIUS Summary.pdf\n",
      "Processing 2018 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2018.md\n",
      "Uploading markdown to S3: processed_markdown/2018.md\n",
      "Processing 2019 CIUS Summary.pdf ...\n",
      "Using absolute PDF path: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2019 CIUS Summary.pdf\n",
      "Processing 2019 CIUS Summary.pdf ...\n",
      "Markdown with embedded images generated in C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\output.md\n",
      "Renaming output to simplified filename: 2019.md\n",
      "Uploading markdown to S3: processed_markdown/2019.md\n",
      "Successfully generated 24 markdown files\n"
     ]
    }
   ],
   "source": [
    "# --- Convert PDFs to Markdown (Mistral) ---\n",
    "print(\"Converting PDFs to Markdown...\")\n",
    "md_files = []\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    try:\n",
    "        pdf_filename = os.path.basename(pdf_path)\n",
    "        print(f\"Processing {pdf_filename} ...\")\n",
    "        \n",
    "        # First extract year from PDF filename\n",
    "        year = extract_year_from_filename(pdf_filename)\n",
    "        if not year:\n",
    "            print(f\"Warning: Could not extract year from filename {pdf_filename}\")\n",
    "            # Use a timestamp if no year is found\n",
    "            year = datetime.now().strftime('%Y')\n",
    "            \n",
    "        out_dir = os.path.join(temp_dir, \"mistral_output\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        # Verify the PDF file exists\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "            \n",
    "        # Get absolute path to ensure no path issues\n",
    "        abs_pdf_path = os.path.abspath(pdf_path)\n",
    "        print(f\"Using absolute PDF path: {abs_pdf_path}\")\n",
    "        \n",
    "        # Process the PDF - this always outputs to output.md\n",
    "        md_path = mistral_process_pdf(pdf_path=abs_pdf_path, output_dir=out_dir)\n",
    "        \n",
    "        if os.path.exists(md_path):\n",
    "            # Create a simplified markdown filename using just the year\n",
    "            simplified_md_filename = f\"{year}.md\"\n",
    "            simplified_md_path = os.path.join(out_dir, simplified_md_filename)\n",
    "            \n",
    "            print(f\"Renaming output to simplified filename: {simplified_md_filename}\")\n",
    "            \n",
    "            # Copy the content to the simplified filename\n",
    "            with open(md_path, 'r', encoding='utf-8') as src:\n",
    "                content = src.read()\n",
    "                \n",
    "            with open(simplified_md_path, 'w', encoding='utf-8') as dest:\n",
    "                dest.write(content)\n",
    "                \n",
    "            # Add the simplified path to our list\n",
    "            md_files.append(simplified_md_path)\n",
    "            \n",
    "            # Upload markdown to S3 with the simplified name\n",
    "            s3_md_key = f\"{markdown_folder}{simplified_md_filename}\"\n",
    "            print(f\"Uploading markdown to S3: {s3_md_key}\")\n",
    "            s3.upload_file(simplified_md_path, bucket, s3_md_key)\n",
    "        else:\n",
    "            print(f\"Warning: Markdown file not created at expected path {md_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {pdf_filename}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Create an error markdown file\n",
    "        error_md_filename = f\"{year}.md\"\n",
    "        error_md_path = os.path.join(out_dir, error_md_filename)\n",
    "        \n",
    "        with open(error_md_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# Processing Error\\n\\nFailed to process {pdf_path}\\n\\nError: {str(e)}\")\n",
    "        \n",
    "        md_files.append(error_md_path)\n",
    "        \n",
    "        # Upload error markdown to S3\n",
    "        s3_md_key = f\"{markdown_folder}{error_md_filename}\"\n",
    "        print(f\"Uploading error markdown to S3: {s3_md_key}\")\n",
    "        s3.upload_file(error_md_path, bucket, s3_md_key)\n",
    "\n",
    "if not md_files:\n",
    "    print(\"No Markdown files generated.\")\n",
    "else:\n",
    "    print(f\"Successfully generated {len(md_files)} markdown files\")\n",
    "\n",
    "# Save the markdown file paths for later processing\n",
    "markdown_paths = {\n",
    "    \"md_files\": md_files,\n",
    "    \"timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "}\n",
    "\n",
    "with open(\"markdown_paths.json\", \"w\") as f:\n",
    "    json.dump(markdown_paths, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Complete\n",
    "\n",
    "The PDFs have been downloaded and converted to markdown files. These files have been uploaded to S3.\n",
    "\n",
    "To proceed with chunking and vector embedding, run the `embedding_and_chunking.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markdown Chunking and Vector Embedding \n",
    "Now that we have successfully converted the pdf files into markdowns ,we will be chunking it into json files for creating vectors and store it into pinecone index `crime-reports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking and creating vector embeddings...\n",
      "Loaded 24 markdown files from saved paths\n"
     ]
    }
   ],
   "source": [
    "print(\"Chunking and creating vector embeddings...\")\n",
    "\n",
    "# Check if we have markdown files\n",
    "try:\n",
    "    with open(\"markdown_paths.json\", \"r\") as f:\n",
    "        markdown_paths = json.load(f)\n",
    "        md_files = markdown_paths.get(\"md_files\", [])\n",
    "        \n",
    "    # Verify these files still exist\n",
    "    md_files = [f for f in md_files if os.path.exists(f)]\n",
    "    print(f\"Loaded {len(md_files)} markdown files from saved paths\")\n",
    "    \n",
    "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "    print(f\"Could not load markdown paths: {e}\")\n",
    "    \n",
    "    # If no markdown files were loaded, list them from S3\n",
    "    print(\"Downloading markdown files from S3...\")\n",
    "    resp = s3.list_objects_v2(Bucket=bucket, Prefix=markdown_folder)\n",
    "    md_files = []\n",
    "    \n",
    "    for obj in resp.get(\"Contents\", []):\n",
    "        if obj[\"Key\"].endswith(\".md\"):\n",
    "            filename = os.path.basename(obj[\"Key\"])\n",
    "            local_md = os.path.join(temp_dir, filename)\n",
    "            s3.download_file(bucket, obj[\"Key\"], local_md)\n",
    "            md_files.append(local_md)\n",
    "            print(f\"Downloaded: {filename}\")\n",
    "            \n",
    "    print(f\"Downloaded {len(md_files)} markdown files from S3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the markdown files are there or not !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following markdown files:\n",
      "1. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\1995.md\n",
      "2. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\1996.md\n",
      "3. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\1997.md\n",
      "4. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\1998.md\n",
      "5. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\1999.md\n",
      "6. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2000.md\n",
      "7. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2001.md\n",
      "8. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2002.md\n",
      "9. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2003.md\n",
      "10. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2004.md\n",
      "11. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2006.md\n",
      "12. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2007.md\n",
      "13. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2008.md\n",
      "14. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2009.md\n",
      "15. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2010.md\n",
      "16. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2011.md\n",
      "17. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2012.md\n",
      "18. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2013.md\n",
      "19. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2014.md\n",
      "20. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2015.md\n",
      "21. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2016.md\n",
      "22. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2017.md\n",
      "23. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2018.md\n",
      "24. C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\mistral_output\\2019.md\n"
     ]
    }
   ],
   "source": [
    "if not md_files:\n",
    "    print(\"No markdown files found!\")\n",
    "else:\n",
    "    # Print all files for debugging\n",
    "    print(\"Found the following markdown files:\")\n",
    "    for i, file in enumerate(md_files):\n",
    "        print(f\"{i+1}. {file}\")\n",
    "        \n",
    "    # Initialize model and chunker\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    chunker = KamradtModifiedChunker(\n",
    "        avg_chunk_size=300,\n",
    "        min_chunk_size=50,\n",
    "        embedding_function=lambda texts: [model.encode(t).tolist() for t in texts]\n",
    "    )\n",
    "    \n",
    "    # Track overall statistics\n",
    "    total_processed = 0\n",
    "    total_failed = 0\n",
    "    processed_files = []\n",
    "    failed_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing markdown files and creating chunks...\n",
      "\n",
      "===== PROCESSING 1995.md =====\n",
      "Year detected: 1995\n",
      "Created 10 chunks\n",
      "Creating chunk part 1 with 2 chunks\n",
      "Creating chunk part 2 with 2 chunks\n",
      "Creating chunk part 3 with 2 chunks\n",
      "Creating chunk part 4 with 2 chunks\n",
      "Creating chunk part 5 with 2 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\1995_chunks.json\n",
      "Uploading chunks to S3: chunks/1995_chunks.json\n",
      "S3 upload successful: chunks/1995_chunks.json\n",
      "✅ Successfully processed chunks for '1995.md'\n",
      "\n",
      "===== PROCESSING 1996.md =====\n",
      "Year detected: 1996\n",
      "Created 11 chunks\n",
      "Creating chunk part 1 with 3 chunks\n",
      "Creating chunk part 2 with 3 chunks\n",
      "Creating chunk part 3 with 3 chunks\n",
      "Creating chunk part 4 with 2 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\1996_chunks.json\n",
      "Uploading chunks to S3: chunks/1996_chunks.json\n",
      "S3 upload successful: chunks/1996_chunks.json\n",
      "✅ Successfully processed chunks for '1996.md'\n",
      "\n",
      "===== PROCESSING 1997.md =====\n",
      "Year detected: 1997\n",
      "Created 13 chunks\n",
      "Creating chunk part 1 with 3 chunks\n",
      "Creating chunk part 2 with 3 chunks\n",
      "Creating chunk part 3 with 3 chunks\n",
      "Creating chunk part 4 with 3 chunks\n",
      "Creating chunk part 5 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\1997_chunks.json\n",
      "Uploading chunks to S3: chunks/1997_chunks.json\n",
      "S3 upload successful: chunks/1997_chunks.json\n",
      "✅ Successfully processed chunks for '1997.md'\n",
      "\n",
      "===== PROCESSING 1998.md =====\n",
      "Year detected: 1998\n",
      "Created 10 chunks\n",
      "Creating chunk part 1 with 2 chunks\n",
      "Creating chunk part 2 with 2 chunks\n",
      "Creating chunk part 3 with 2 chunks\n",
      "Creating chunk part 4 with 2 chunks\n",
      "Creating chunk part 5 with 2 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\1998_chunks.json\n",
      "Uploading chunks to S3: chunks/1998_chunks.json\n",
      "S3 upload successful: chunks/1998_chunks.json\n",
      "✅ Successfully processed chunks for '1998.md'\n",
      "\n",
      "===== PROCESSING 1999.md =====\n",
      "Year detected: 1999\n",
      "Created 10 chunks\n",
      "Creating chunk part 1 with 2 chunks\n",
      "Creating chunk part 2 with 2 chunks\n",
      "Creating chunk part 3 with 2 chunks\n",
      "Creating chunk part 4 with 2 chunks\n",
      "Creating chunk part 5 with 2 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\1999_chunks.json\n",
      "Uploading chunks to S3: chunks/1999_chunks.json\n",
      "S3 upload successful: chunks/1999_chunks.json\n",
      "✅ Successfully processed chunks for '1999.md'\n",
      "\n",
      "===== PROCESSING 2000.md =====\n",
      "Year detected: 2000\n",
      "Created 10 chunks\n",
      "Creating chunk part 1 with 2 chunks\n",
      "Creating chunk part 2 with 2 chunks\n",
      "Creating chunk part 3 with 2 chunks\n",
      "Creating chunk part 4 with 2 chunks\n",
      "Creating chunk part 5 with 2 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2000_chunks.json\n",
      "Uploading chunks to S3: chunks/2000_chunks.json\n",
      "S3 upload successful: chunks/2000_chunks.json\n",
      "✅ Successfully processed chunks for '2000.md'\n",
      "\n",
      "===== PROCESSING 2001.md =====\n",
      "Year detected: 2001\n",
      "Created 13 chunks\n",
      "Creating chunk part 1 with 3 chunks\n",
      "Creating chunk part 2 with 3 chunks\n",
      "Creating chunk part 3 with 3 chunks\n",
      "Creating chunk part 4 with 3 chunks\n",
      "Creating chunk part 5 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2001_chunks.json\n",
      "Uploading chunks to S3: chunks/2001_chunks.json\n",
      "S3 upload successful: chunks/2001_chunks.json\n",
      "✅ Successfully processed chunks for '2001.md'\n",
      "\n",
      "===== PROCESSING 2002.md =====\n",
      "Year detected: 2002\n",
      "Created 9 chunks\n",
      "Creating chunk part 1 with 2 chunks\n",
      "Creating chunk part 2 with 2 chunks\n",
      "Creating chunk part 3 with 2 chunks\n",
      "Creating chunk part 4 with 2 chunks\n",
      "Creating chunk part 5 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2002_chunks.json\n",
      "Uploading chunks to S3: chunks/2002_chunks.json\n",
      "S3 upload successful: chunks/2002_chunks.json\n",
      "✅ Successfully processed chunks for '2002.md'\n",
      "\n",
      "===== PROCESSING 2003.md =====\n",
      "Year detected: 2003\n",
      "Created 11 chunks\n",
      "Creating chunk part 1 with 3 chunks\n",
      "Creating chunk part 2 with 3 chunks\n",
      "Creating chunk part 3 with 3 chunks\n",
      "Creating chunk part 4 with 2 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2003_chunks.json\n",
      "Uploading chunks to S3: chunks/2003_chunks.json\n",
      "S3 upload successful: chunks/2003_chunks.json\n",
      "✅ Successfully processed chunks for '2003.md'\n",
      "\n",
      "===== PROCESSING 2004.md =====\n",
      "Year detected: 2004\n",
      "Created 658 chunks\n",
      "Creating chunk part 1 with 132 chunks\n",
      "Creating chunk part 2 with 132 chunks\n",
      "Creating chunk part 3 with 132 chunks\n",
      "Creating chunk part 4 with 132 chunks\n",
      "Creating chunk part 5 with 130 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2004_chunks.json\n",
      "Uploading chunks to S3: chunks/2004_chunks.json\n",
      "S3 upload successful: chunks/2004_chunks.json\n",
      "✅ Successfully processed chunks for '2004.md'\n",
      "\n",
      "===== PROCESSING 2006.md =====\n",
      "Year detected: 2006\n",
      "Created 4 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Creating chunk part 4 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2006_chunks.json\n",
      "Uploading chunks to S3: chunks/2006_chunks.json\n",
      "S3 upload successful: chunks/2006_chunks.json\n",
      "✅ Successfully processed chunks for '2006.md'\n",
      "\n",
      "===== PROCESSING 2007.md =====\n",
      "Year detected: 2007\n",
      "Created 1 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2007_chunks.json\n",
      "Uploading chunks to S3: chunks/2007_chunks.json\n",
      "S3 upload successful: chunks/2007_chunks.json\n",
      "✅ Successfully processed chunks for '2007.md'\n",
      "\n",
      "===== PROCESSING 2008.md =====\n",
      "Year detected: 2008\n",
      "Created 3 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2008_chunks.json\n",
      "Uploading chunks to S3: chunks/2008_chunks.json\n",
      "S3 upload successful: chunks/2008_chunks.json\n",
      "✅ Successfully processed chunks for '2008.md'\n",
      "\n",
      "===== PROCESSING 2009.md =====\n",
      "Year detected: 2009\n",
      "Created 4 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Creating chunk part 4 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2009_chunks.json\n",
      "Uploading chunks to S3: chunks/2009_chunks.json\n",
      "S3 upload successful: chunks/2009_chunks.json\n",
      "✅ Successfully processed chunks for '2009.md'\n",
      "\n",
      "===== PROCESSING 2010.md =====\n",
      "Year detected: 2010\n",
      "Created 3 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2010_chunks.json\n",
      "Uploading chunks to S3: chunks/2010_chunks.json\n",
      "S3 upload successful: chunks/2010_chunks.json\n",
      "✅ Successfully processed chunks for '2010.md'\n",
      "\n",
      "===== PROCESSING 2011.md =====\n",
      "Year detected: 2011\n",
      "Created 1 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2011_chunks.json\n",
      "Uploading chunks to S3: chunks/2011_chunks.json\n",
      "S3 upload successful: chunks/2011_chunks.json\n",
      "✅ Successfully processed chunks for '2011.md'\n",
      "\n",
      "===== PROCESSING 2012.md =====\n",
      "Year detected: 2012\n",
      "Created 3 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2012_chunks.json\n",
      "Uploading chunks to S3: chunks/2012_chunks.json\n",
      "S3 upload successful: chunks/2012_chunks.json\n",
      "✅ Successfully processed chunks for '2012.md'\n",
      "\n",
      "===== PROCESSING 2013.md =====\n",
      "Year detected: 2013\n",
      "Created 4 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Creating chunk part 4 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2013_chunks.json\n",
      "Uploading chunks to S3: chunks/2013_chunks.json\n",
      "S3 upload successful: chunks/2013_chunks.json\n",
      "✅ Successfully processed chunks for '2013.md'\n",
      "\n",
      "===== PROCESSING 2014.md =====\n",
      "Year detected: 2014\n",
      "Created 4 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Creating chunk part 4 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2014_chunks.json\n",
      "Uploading chunks to S3: chunks/2014_chunks.json\n",
      "S3 upload successful: chunks/2014_chunks.json\n",
      "✅ Successfully processed chunks for '2014.md'\n",
      "\n",
      "===== PROCESSING 2015.md =====\n",
      "Year detected: 2015\n",
      "Created 4 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Creating chunk part 4 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2015_chunks.json\n",
      "Uploading chunks to S3: chunks/2015_chunks.json\n",
      "S3 upload successful: chunks/2015_chunks.json\n",
      "✅ Successfully processed chunks for '2015.md'\n",
      "\n",
      "===== PROCESSING 2016.md =====\n",
      "Year detected: 2016\n",
      "Created 4 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Creating chunk part 4 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2016_chunks.json\n",
      "Uploading chunks to S3: chunks/2016_chunks.json\n",
      "S3 upload successful: chunks/2016_chunks.json\n",
      "✅ Successfully processed chunks for '2016.md'\n",
      "\n",
      "===== PROCESSING 2017.md =====\n",
      "Year detected: 2017\n",
      "Created 4 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Creating chunk part 4 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2017_chunks.json\n",
      "Uploading chunks to S3: chunks/2017_chunks.json\n",
      "S3 upload successful: chunks/2017_chunks.json\n",
      "✅ Successfully processed chunks for '2017.md'\n",
      "\n",
      "===== PROCESSING 2018.md =====\n",
      "Year detected: 2018\n",
      "Created 3 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2018_chunks.json\n",
      "Uploading chunks to S3: chunks/2018_chunks.json\n",
      "S3 upload successful: chunks/2018_chunks.json\n",
      "✅ Successfully processed chunks for '2018.md'\n",
      "\n",
      "===== PROCESSING 2019.md =====\n",
      "Year detected: 2019\n",
      "Created 3 chunks\n",
      "Creating chunk part 1 with 1 chunks\n",
      "Creating chunk part 2 with 1 chunks\n",
      "Creating chunk part 3 with 1 chunks\n",
      "Successfully created local chunks file: C:\\Users\\shush\\AppData\\Local\\Temp\\pdf_downloads_3fpo62tc\\2019_chunks.json\n",
      "Uploading chunks to S3: chunks/2019_chunks.json\n",
      "S3 upload successful: chunks/2019_chunks.json\n",
      "✅ Successfully processed chunks for '2019.md'\n",
      "\n",
      "===== CHUNK PROCESSING SUMMARY =====\n",
      "Total files processed successfully: 24\n",
      "Total files failed: 0\n",
      "\n",
      "All markdown files chunked and uploaded to S3!\n"
     ]
    }
   ],
   "source": [
    "# --- Create and upload chunks to S3 ---\n",
    "import json, uuid, os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "print(\"Processing markdown files and creating chunks...\")\n",
    "\n",
    "# Track overall statistics\n",
    "total_processed = 0\n",
    "total_failed = 0\n",
    "processed_files = []\n",
    "failed_files = []\n",
    "all_chunk_data = {}  # Store chunk data for later vector embedding\n",
    "\n",
    "# Define number of parts to split chunks into\n",
    "PARTS_PER_FILE = 5  # You can adjust this number based on your needs\n",
    "\n",
    "# Process each markdown file for chunking\n",
    "for md_file in md_files:\n",
    "    try:\n",
    "        # Validate file exists\n",
    "        if not os.path.exists(md_file):\n",
    "            print(f\"WARNING: File not found: {md_file}\")\n",
    "            total_failed += 1\n",
    "            failed_files.append(md_file)\n",
    "            continue\n",
    "            \n",
    "        with open(md_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Get the filename to extract year\n",
    "        md_filename = os.path.basename(md_file)\n",
    "        print(f\"\\n===== PROCESSING {md_filename} =====\")\n",
    "        \n",
    "        # Extract year from the filename\n",
    "        year = os.path.splitext(md_filename)[0]\n",
    "        if not re.match(r'^(19|20)\\d{2}$', year):\n",
    "            year = extract_year_from_filename(md_filename)\n",
    "            if not year:\n",
    "                year = extract_year_from_content(content)\n",
    "                \n",
    "        print(f\"Year detected: {year}\")\n",
    "                \n",
    "        # Process chunks\n",
    "        chunks = chunker.split_text(content)\n",
    "        total_chunks = len(chunks)\n",
    "        print(f\"Created {total_chunks} chunks\")\n",
    "        \n",
    "        # Calculate chunks per part\n",
    "        chunks_per_part = math.ceil(total_chunks / PARTS_PER_FILE)\n",
    "        \n",
    "        # Split chunks into parts\n",
    "        chunk_parts = []\n",
    "        for i in range(0, total_chunks, chunks_per_part):\n",
    "            part_chunks = chunks[i:i + chunks_per_part]\n",
    "            timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')\n",
    "            file_uuid = str(uuid.uuid4())[:8]\n",
    "            \n",
    "            part_data = {\n",
    "                \"part_id\": f\"part{len(chunk_parts) + 1}_{timestamp}_{file_uuid}\",\n",
    "                \"chunks\": part_chunks,\n",
    "                \"chunk_count\": len(part_chunks),\n",
    "                \"source_file\": md_filename,\n",
    "                \"processed_date\": datetime.now(timezone.utc).isoformat()\n",
    "            }\n",
    "            chunk_parts.append(part_data)\n",
    "            print(f\"Creating chunk part {len(chunk_parts)} with {len(part_chunks)} chunks\")\n",
    "        \n",
    "        # Create the main chunks JSON structure\n",
    "        chunks_json_filename = f\"{year}_chunks.json\"\n",
    "        chunks_json_path = os.path.join(temp_dir, chunks_json_filename)\n",
    "        s3_chunks_key = f\"{chunks_folder}{chunks_json_filename}\"\n",
    "        \n",
    "        # Create the complete chunks data structure\n",
    "        chunks_data = {\n",
    "            \"year\": year,\n",
    "            \"total_chunks\": total_chunks,\n",
    "            \"total_parts\": len(chunk_parts),\n",
    "            \"document_id\": f\"{year}_{str(uuid.uuid4())[:8]}\",\n",
    "            \"created_by\": \"shushilgirish\",\n",
    "            \"created_date\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"source_files\": [md_filename],\n",
    "            \"s3_location\": s3_chunks_key,\n",
    "            \"parts\": chunk_parts,\n",
    "            \"metadata\": {\n",
    "                \"chunks_version\": \"1.0\",\n",
    "                \"processing_timestamp\": datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save locally first\n",
    "        with open(chunks_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        # Verify the file was created locally\n",
    "        if not os.path.exists(chunks_json_path):\n",
    "            print(f\"ERROR: Failed to create local chunks file: {chunks_json_path}\")\n",
    "            raise Exception(\"Local file creation failed\")\n",
    "        else:\n",
    "            print(f\"Successfully created local chunks file: {chunks_json_path}\")\n",
    "            \n",
    "        # Upload chunks JSON to S3\n",
    "        print(f\"Uploading chunks to S3: {s3_chunks_key}\")\n",
    "        s3.upload_file(chunks_json_path, bucket, s3_chunks_key)\n",
    "        \n",
    "        # Verify the S3 upload\n",
    "        try:\n",
    "            s3.head_object(Bucket=bucket, Key=s3_chunks_key)\n",
    "            print(f\"S3 upload successful: {s3_chunks_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: S3 upload verification failed: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "        # Store data for later vector embedding\n",
    "        all_chunk_data[md_file] = {\n",
    "            \"year\": year,\n",
    "            \"total_chunks\": total_chunks,\n",
    "            \"total_parts\": len(chunk_parts),\n",
    "            \"s3_chunks_key\": s3_chunks_key,\n",
    "            \"md_filename\": md_filename,\n",
    "            \"document_id\": chunks_data[\"document_id\"]\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ Successfully processed chunks for '{md_filename}'\")\n",
    "        total_processed += 1\n",
    "        processed_files.append(md_file)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR processing markdown file {md_file}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        total_failed += 1\n",
    "        failed_files.append(md_file)\n",
    "\n",
    "# Print summary of chunk processing\n",
    "print(\"\\n===== CHUNK PROCESSING SUMMARY =====\")\n",
    "print(f\"Total files processed successfully: {total_processed}\")\n",
    "print(f\"Total files failed: {total_failed}\")\n",
    "\n",
    "if failed_files:\n",
    "    print(\"\\nFailed files:\")\n",
    "    for f in failed_files:\n",
    "        print(f\"- {f}\")\n",
    "\n",
    "print(\"\\nAll markdown files chunked and uploaded to S3!\")\n",
    "\n",
    "# Save chunk data for the next cell\n",
    "with open(\"chunk_data.json\", \"w\") as f:\n",
    "    # Convert to serializable format\n",
    "    serializable_data = {}\n",
    "    for md_file, data in all_chunk_data.items():\n",
    "        serializable_data[md_file] = {\n",
    "            \"year\": data[\"year\"],\n",
    "            \"total_chunks\": data[\"total_chunks\"],\n",
    "            \"total_parts\": data[\"total_parts\"],\n",
    "            \"s3_chunks_key\": data[\"s3_chunks_key\"],\n",
    "            \"md_filename\": data[\"md_filename\"],\n",
    "            \"document_id\": data[\"document_id\"]\n",
    "        }\n",
    "    json.dump(serializable_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embedding and Pinecone Upload (Separate Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and uploading to Pinecone...\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 1995.md =====\n",
      "Year for embedding: 1995\n",
      "Successfully loaded chunks from S3: chunks/1995_chunks.json\n",
      "Processing part part1_20250402_153117_42e9729e with 2 chunks\n",
      "Processing part part2_20250402_153117_c1bc7623 with 2 chunks\n",
      "Processing part part3_20250402_153117_245f800e with 2 chunks\n",
      "Processing part part4_20250402_153117_28d95744 with 2 chunks\n",
      "Processing part part5_20250402_153117_2a3dfa8f with 2 chunks\n",
      "Upserting 10 vectors to Pinecone namespace: 1995\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 10 chunks for '1995.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 1996.md =====\n",
      "Year for embedding: 1996\n",
      "Successfully loaded chunks from S3: chunks/1996_chunks.json\n",
      "Processing part part1_20250402_153117_00187691 with 3 chunks\n",
      "Processing part part2_20250402_153117_653ae898 with 3 chunks\n",
      "Processing part part3_20250402_153117_c948b383 with 3 chunks\n",
      "Processing part part4_20250402_153117_1c931454 with 2 chunks\n",
      "Upserting 11 vectors to Pinecone namespace: 1996\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 11 chunks for '1996.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 1997.md =====\n",
      "Year for embedding: 1997\n",
      "Successfully loaded chunks from S3: chunks/1997_chunks.json\n",
      "Processing part part1_20250402_153117_f3a5ac94 with 3 chunks\n",
      "Processing part part2_20250402_153117_932f1813 with 3 chunks\n",
      "Processing part part3_20250402_153117_c30f5148 with 3 chunks\n",
      "Processing part part4_20250402_153117_09928c41 with 3 chunks\n",
      "Processing part part5_20250402_153117_97c1d384 with 1 chunks\n",
      "Upserting 13 vectors to Pinecone namespace: 1997\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 13 chunks for '1997.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 1998.md =====\n",
      "Year for embedding: 1998\n",
      "Successfully loaded chunks from S3: chunks/1998_chunks.json\n",
      "Processing part part1_20250402_153117_52171c23 with 2 chunks\n",
      "Processing part part2_20250402_153117_427afebc with 2 chunks\n",
      "Processing part part3_20250402_153117_f17a5ae4 with 2 chunks\n",
      "Processing part part4_20250402_153117_d3f44235 with 2 chunks\n",
      "Processing part part5_20250402_153117_4397e1b8 with 2 chunks\n",
      "Upserting 10 vectors to Pinecone namespace: 1998\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 10 chunks for '1998.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 1999.md =====\n",
      "Year for embedding: 1999\n",
      "Successfully loaded chunks from S3: chunks/1999_chunks.json\n",
      "Processing part part1_20250402_153118_608e2faf with 2 chunks\n",
      "Processing part part2_20250402_153118_032fec82 with 2 chunks\n",
      "Processing part part3_20250402_153118_520aaa2b with 2 chunks\n",
      "Processing part part4_20250402_153118_8a0c8c1e with 2 chunks\n",
      "Processing part part5_20250402_153118_53363104 with 2 chunks\n",
      "Upserting 10 vectors to Pinecone namespace: 1999\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 10 chunks for '1999.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2000.md =====\n",
      "Year for embedding: 2000\n",
      "Successfully loaded chunks from S3: chunks/2000_chunks.json\n",
      "Processing part part1_20250402_153118_85c2f785 with 2 chunks\n",
      "Processing part part2_20250402_153118_a2b8aaf6 with 2 chunks\n",
      "Processing part part3_20250402_153118_baf83e79 with 2 chunks\n",
      "Processing part part4_20250402_153118_3489b681 with 2 chunks\n",
      "Processing part part5_20250402_153118_114813ea with 2 chunks\n",
      "Upserting 10 vectors to Pinecone namespace: 2000\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 10 chunks for '2000.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2001.md =====\n",
      "Year for embedding: 2001\n",
      "Successfully loaded chunks from S3: chunks/2001_chunks.json\n",
      "Processing part part1_20250402_153118_8d9da6f3 with 3 chunks\n",
      "Processing part part2_20250402_153118_5f9bad88 with 3 chunks\n",
      "Processing part part3_20250402_153118_c48ceb78 with 3 chunks\n",
      "Processing part part4_20250402_153118_14cd9ab1 with 3 chunks\n",
      "Processing part part5_20250402_153118_a88017b0 with 1 chunks\n",
      "Upserting 13 vectors to Pinecone namespace: 2001\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 13 chunks for '2001.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2002.md =====\n",
      "Year for embedding: 2002\n",
      "Successfully loaded chunks from S3: chunks/2002_chunks.json\n",
      "Processing part part1_20250402_153118_bdb159df with 2 chunks\n",
      "Processing part part2_20250402_153118_c607c222 with 2 chunks\n",
      "Processing part part3_20250402_153118_623cd701 with 2 chunks\n",
      "Processing part part4_20250402_153118_1efc505f with 2 chunks\n",
      "Processing part part5_20250402_153118_348e7c18 with 1 chunks\n",
      "Upserting 9 vectors to Pinecone namespace: 2002\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 9 chunks for '2002.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2003.md =====\n",
      "Year for embedding: 2003\n",
      "Successfully loaded chunks from S3: chunks/2003_chunks.json\n",
      "Processing part part1_20250402_153119_86b38951 with 3 chunks\n",
      "Processing part part2_20250402_153119_f6e065d4 with 3 chunks\n",
      "Processing part part3_20250402_153119_39a3f05a with 3 chunks\n",
      "Processing part part4_20250402_153119_389bd33f with 2 chunks\n",
      "Upserting 11 vectors to Pinecone namespace: 2003\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 11 chunks for '2003.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2004.md =====\n",
      "Year for embedding: 2004\n",
      "Successfully loaded chunks from S3: chunks/2004_chunks.json\n",
      "Processing part part1_20250402_153119_82f60cd0 with 132 chunks\n",
      "Processing part part2_20250402_153119_c3d0f555 with 132 chunks\n",
      "Processing part part3_20250402_153119_468785ad with 132 chunks\n",
      "Processing part part4_20250402_153119_8c4c849c with 132 chunks\n",
      "Processing part part5_20250402_153119_1fbec41b with 130 chunks\n",
      "Upserting 658 vectors to Pinecone namespace: 2004\n",
      "Successfully upserted batch 1/7\n",
      "Successfully upserted batch 2/7\n",
      "Successfully upserted batch 3/7\n",
      "Successfully upserted batch 4/7\n",
      "Successfully upserted batch 5/7\n",
      "Successfully upserted batch 6/7\n",
      "Successfully upserted batch 7/7\n",
      "✅ Successfully embedded 658 chunks for '2004.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2006.md =====\n",
      "Year for embedding: 2006\n",
      "Successfully loaded chunks from S3: chunks/2006_chunks.json\n",
      "Processing part part1_20250402_153121_39efd215 with 1 chunks\n",
      "Processing part part2_20250402_153121_0cf6561d with 1 chunks\n",
      "Processing part part3_20250402_153121_55804a10 with 1 chunks\n",
      "Processing part part4_20250402_153121_9be2b603 with 1 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2006\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 4 chunks for '2006.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2007.md =====\n",
      "Year for embedding: 2007\n",
      "Successfully loaded chunks from S3: chunks/2007_chunks.json\n",
      "Processing part part1_20250402_153121_f2e3e9b4 with 1 chunks\n",
      "Upserting 1 vectors to Pinecone namespace: 2007\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 1 chunks for '2007.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2008.md =====\n",
      "Year for embedding: 2008\n",
      "Successfully loaded chunks from S3: chunks/2008_chunks.json\n",
      "Processing part part1_20250402_153121_a4ba1715 with 1 chunks\n",
      "Processing part part2_20250402_153121_8a136e6f with 1 chunks\n",
      "Processing part part3_20250402_153121_dccf7458 with 1 chunks\n",
      "Upserting 3 vectors to Pinecone namespace: 2008\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 3 chunks for '2008.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2009.md =====\n",
      "Year for embedding: 2009\n",
      "Successfully loaded chunks from S3: chunks/2009_chunks.json\n",
      "Processing part part1_20250402_153121_6b34128b with 1 chunks\n",
      "Processing part part2_20250402_153121_9c81c726 with 1 chunks\n",
      "Processing part part3_20250402_153121_baac2512 with 1 chunks\n",
      "Processing part part4_20250402_153121_64f81791 with 1 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2009\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 4 chunks for '2009.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2010.md =====\n",
      "Year for embedding: 2010\n",
      "Successfully loaded chunks from S3: chunks/2010_chunks.json\n",
      "Processing part part1_20250402_153121_63f92ba9 with 1 chunks\n",
      "Processing part part2_20250402_153121_d066111c with 1 chunks\n",
      "Processing part part3_20250402_153121_f1d4b29d with 1 chunks\n",
      "Upserting 3 vectors to Pinecone namespace: 2010\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 3 chunks for '2010.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2011.md =====\n",
      "Year for embedding: 2011\n",
      "Successfully loaded chunks from S3: chunks/2011_chunks.json\n",
      "Processing part part1_20250402_153121_8d11f3c4 with 1 chunks\n",
      "Upserting 1 vectors to Pinecone namespace: 2011\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 1 chunks for '2011.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2012.md =====\n",
      "Year for embedding: 2012\n",
      "Successfully loaded chunks from S3: chunks/2012_chunks.json\n",
      "Processing part part1_20250402_153122_77059d70 with 1 chunks\n",
      "Processing part part2_20250402_153122_9bf87a97 with 1 chunks\n",
      "Processing part part3_20250402_153122_7ce541b4 with 1 chunks\n",
      "Upserting 3 vectors to Pinecone namespace: 2012\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 3 chunks for '2012.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2013.md =====\n",
      "Year for embedding: 2013\n",
      "Successfully loaded chunks from S3: chunks/2013_chunks.json\n",
      "Processing part part1_20250402_153122_01ebf974 with 1 chunks\n",
      "Processing part part2_20250402_153122_63cd4a19 with 1 chunks\n",
      "Processing part part3_20250402_153122_398fbe2f with 1 chunks\n",
      "Processing part part4_20250402_153122_a1db9aeb with 1 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2013\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 4 chunks for '2013.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2014.md =====\n",
      "Year for embedding: 2014\n",
      "Successfully loaded chunks from S3: chunks/2014_chunks.json\n",
      "Processing part part1_20250402_153122_76f72365 with 1 chunks\n",
      "Processing part part2_20250402_153122_37f70149 with 1 chunks\n",
      "Processing part part3_20250402_153122_700d5b56 with 1 chunks\n",
      "Processing part part4_20250402_153122_177f3bde with 1 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2014\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 4 chunks for '2014.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2015.md =====\n",
      "Year for embedding: 2015\n",
      "Successfully loaded chunks from S3: chunks/2015_chunks.json\n",
      "Processing part part1_20250402_153122_1b19b689 with 1 chunks\n",
      "Processing part part2_20250402_153122_979710d5 with 1 chunks\n",
      "Processing part part3_20250402_153122_5ae3893a with 1 chunks\n",
      "Processing part part4_20250402_153122_40d226bd with 1 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2015\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 4 chunks for '2015.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2016.md =====\n",
      "Year for embedding: 2016\n",
      "Successfully loaded chunks from S3: chunks/2016_chunks.json\n",
      "Processing part part1_20250402_153122_eabc05b4 with 1 chunks\n",
      "Processing part part2_20250402_153122_e6eacbd0 with 1 chunks\n",
      "Processing part part3_20250402_153122_72e5b966 with 1 chunks\n",
      "Processing part part4_20250402_153122_1c3692af with 1 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2016\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 4 chunks for '2016.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2017.md =====\n",
      "Year for embedding: 2017\n",
      "Successfully loaded chunks from S3: chunks/2017_chunks.json\n",
      "Processing part part1_20250402_153122_e9bf0b74 with 1 chunks\n",
      "Processing part part2_20250402_153122_bbec8fc0 with 1 chunks\n",
      "Processing part part3_20250402_153122_a1ffc39c with 1 chunks\n",
      "Processing part part4_20250402_153122_a31bdf88 with 1 chunks\n",
      "Upserting 4 vectors to Pinecone namespace: 2017\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 4 chunks for '2017.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2018.md =====\n",
      "Year for embedding: 2018\n",
      "Successfully loaded chunks from S3: chunks/2018_chunks.json\n",
      "Processing part part1_20250402_153122_3ad8d661 with 1 chunks\n",
      "Processing part part2_20250402_153122_fddd7e76 with 1 chunks\n",
      "Processing part part3_20250402_153122_e08c53db with 1 chunks\n",
      "Upserting 3 vectors to Pinecone namespace: 2018\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 3 chunks for '2018.md'\n",
      "\n",
      "===== CREATING EMBEDDINGS FOR 2019.md =====\n",
      "Year for embedding: 2019\n",
      "Successfully loaded chunks from S3: chunks/2019_chunks.json\n",
      "Processing part part1_20250402_153123_76616a67 with 1 chunks\n",
      "Processing part part2_20250402_153123_46e4b966 with 1 chunks\n",
      "Processing part part3_20250402_153123_dd43f194 with 1 chunks\n",
      "Upserting 3 vectors to Pinecone namespace: 2019\n",
      "Successfully upserted batch 1/1\n",
      "✅ Successfully embedded 3 chunks for '2019.md'\n",
      "\n",
      "===== EMBEDDING SUMMARY =====\n",
      "Total files embedded: 24\n",
      "Total vectors created: 800\n",
      "Embedding errors: 0\n",
      "\n",
      "Vector embedding complete!\n"
     ]
    }
   ],
   "source": [
    "# --- Vector Embedding and Pinecone Upload ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json, uuid\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "import math\n",
    "print(\"Creating embeddings and uploading to Pinecone...\")\n",
    "\n",
    "# Initialize model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Make sure we have the index initialized\n",
    "if 'index' not in locals():\n",
    "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "    index = pc.Index(\"crime-reports\")\n",
    "\n",
    "# Track embedding progress\n",
    "total_vectors = 0\n",
    "total_files_embedded = 0\n",
    "embedding_errors = 0\n",
    "\n",
    "# Load chunk data from previous processing\n",
    "try:\n",
    "    with open(\"chunk_data.json\", \"r\") as f:\n",
    "        chunk_data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading chunk_data.json: {str(e)}\")\n",
    "    chunk_data = {}\n",
    "\n",
    "# Process each markdown file\n",
    "for md_file in md_files:\n",
    "    try:\n",
    "        # Get the filename\n",
    "        md_filename = os.path.basename(md_file)\n",
    "        print(f\"\\n===== CREATING EMBEDDINGS FOR {md_filename} =====\")\n",
    "        \n",
    "        # Get file data from chunk_data.json\n",
    "        if md_file not in chunk_data:\n",
    "            print(f\"No chunk data found for {md_file}, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        file_data = chunk_data[md_file]\n",
    "        year = file_data[\"year\"]\n",
    "        s3_chunks_key = file_data[\"s3_chunks_key\"]\n",
    "        \n",
    "        print(f\"Year for embedding: {year}\")\n",
    "        \n",
    "        # Get chunks from S3\n",
    "        try:\n",
    "            response = s3.get_object(Bucket=bucket, Key=s3_chunks_key)\n",
    "            chunks_data = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            print(f\"Successfully loaded chunks from S3: {s3_chunks_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading chunks from S3: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # Create vector embeddings for each part\n",
    "        vectors = []\n",
    "        total_chunks_processed = 0\n",
    "        \n",
    "        for part in chunks_data[\"parts\"]:\n",
    "            part_id = part[\"part_id\"]\n",
    "            chunks = part[\"chunks\"]\n",
    "            \n",
    "            print(f\"Processing part {part_id} with {len(chunks)} chunks\")\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Create embedding\n",
    "                emb = model.encode(chunk).tolist()\n",
    "                \n",
    "                # Create metadata for retrieval\n",
    "                metadata = {\n",
    "                    \"file\": md_filename,\n",
    "                    \"document_id\": chunks_data[\"document_id\"],\n",
    "                    \"part_id\": part_id,\n",
    "                    \"chunk_index\": total_chunks_processed + i,\n",
    "                    \"text_preview\": chunk[:100] + (\"...\" if len(chunk) > 100 else \"\"),\n",
    "                    \"chunks_s3_path\": s3_chunks_key,\n",
    "                    \"s3_bucket\": bucket,\n",
    "                    \"year\": year,\n",
    "                    \"chunk_length\": len(chunk),\n",
    "                    \"time_processed\": datetime.now(timezone.utc).isoformat(),\n",
    "                    \"processed_by\": \"shushilgirish\"\n",
    "                }\n",
    "                \n",
    "                vector_id = f\"{year}_{part_id}_{i}_{uuid.uuid4()}\"\n",
    "                vectors.append((\n",
    "                    vector_id,\n",
    "                    emb,\n",
    "                    metadata\n",
    "                ))\n",
    "            \n",
    "            total_chunks_processed += len(chunks)\n",
    "            \n",
    "        # Upsert vectors in batches\n",
    "        print(f\"Upserting {len(vectors)} vectors to Pinecone namespace: {year}\")\n",
    "        \n",
    "        batch_size = 100\n",
    "        for i in range(0, len(vectors), batch_size):\n",
    "            batch = vectors[i:i + batch_size]\n",
    "            try:\n",
    "                # Use year as namespace for easy filtering\n",
    "                index.upsert(vectors=batch, namespace=str(year))\n",
    "                print(f\"Successfully upserted batch {i//batch_size + 1}/{math.ceil(len(vectors)/batch_size)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error upserting batch to Pinecone: {str(e)}\")\n",
    "                raise\n",
    "        \n",
    "        total_vectors += len(vectors)\n",
    "        total_files_embedded += 1\n",
    "        print(f\"✅ Successfully embedded {len(vectors)} chunks for '{md_filename}'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR creating embeddings for {md_file}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        embedding_errors += 1\n",
    "\n",
    "# Print embedding summary\n",
    "print(\"\\n===== EMBEDDING SUMMARY =====\")\n",
    "print(f\"Total files embedded: {total_files_embedded}\")\n",
    "print(f\"Total vectors created: {total_vectors}\")\n",
    "print(f\"Embedding errors: {embedding_errors}\")\n",
    "print(\"\\nVector embedding complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check if Pinecone index is created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Pinecone connection...\n",
      "Found Pinecone API key: pcsk...\n",
      "\n",
      "Current Pinecone indexes:\n",
      "1. embedding-cosine\n",
      "2. crime-records\n",
      "3. gpt-4o-research-agent\n",
      "4. pinecone-embeddings\n",
      "5. nvidia-reports\n",
      "\n",
      "Success: Index 'crime-records' exists in Pinecone.\n",
      "Connected to index 'crime-records'\n"
     ]
    }
   ],
   "source": [
    "# --- Comprehensive Pinecone Setup ---\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "print(\"Setting up Pinecone connection...\")\n",
    "\n",
    "# Load environment variables if not already loaded\n",
    "if \"PINECONE_API_KEY\" not in os.environ:\n",
    "    load_dotenv()\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "if not pinecone_api_key:\n",
    "    print(\"ERROR: PINECONE_API_KEY not found in environment variables\")\n",
    "    print(\"Please check your .env file or set it manually.\")\n",
    "else:\n",
    "    print(f\"Found Pinecone API key: {pinecone_api_key[:4]}...\")  # Show first 4 chars only for security\n",
    "\n",
    "# Initialize Pinecone client\n",
    "try:\n",
    "    pc = Pinecone(api_key=pinecone_api_key)\n",
    "    \n",
    "    # List all available indexes\n",
    "    all_indexes = pc.list_indexes()\n",
    "    print(\"\\nCurrent Pinecone indexes:\")\n",
    "    if not all_indexes.names():\n",
    "        print(\"No indexes found in this Pinecone account.\")\n",
    "    else:\n",
    "        for i, name in enumerate(all_indexes.names()):\n",
    "            print(f\"{i+1}. {name}\")\n",
    "    \n",
    "    # Set our target index name\n",
    "    index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "    \n",
    "    # Check if the index exists\n",
    "    if index_name in all_indexes.names():\n",
    "        print(f\"\\nSuccess: Index '{index_name}' exists in Pinecone.\")\n",
    "        index = pc.Index(index_name)\n",
    "        print(f\"Connected to index '{index_name}'\")\n",
    "    else:\n",
    "        print(f\"\\nIndex '{index_name}' does not exist in Pinecone. Creating...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR connecting to Pinecone: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index stats: {'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'1995': {'vector_count': 10},\n",
      "                '1996': {'vector_count': 11},\n",
      "                '1997': {'vector_count': 13},\n",
      "                '1998': {'vector_count': 10},\n",
      "                '1999': {'vector_count': 10},\n",
      "                '2000': {'vector_count': 10},\n",
      "                '2001': {'vector_count': 13},\n",
      "                '2002': {'vector_count': 9},\n",
      "                '2003': {'vector_count': 11},\n",
      "                '2004': {'vector_count': 658},\n",
      "                '2006': {'vector_count': 4},\n",
      "                '2007': {'vector_count': 1},\n",
      "                '2008': {'vector_count': 3},\n",
      "                '2009': {'vector_count': 4},\n",
      "                '2010': {'vector_count': 3},\n",
      "                '2011': {'vector_count': 1},\n",
      "                '2012': {'vector_count': 3},\n",
      "                '2013': {'vector_count': 4},\n",
      "                '2014': {'vector_count': 4},\n",
      "                '2015': {'vector_count': 4},\n",
      "                '2016': {'vector_count': 4},\n",
      "                '2017': {'vector_count': 4},\n",
      "                '2018': {'vector_count': 3},\n",
      "                '2019': {'vector_count': 3}},\n",
      " 'total_vector_count': 800,\n",
      " 'vector_type': 'dense'}\n",
      "Total vectors in index: 800\n"
     ]
    }
   ],
   "source": [
    "stats = index.describe_index_stats()\n",
    "print(f\"\\nIndex stats: {stats}\")\n",
    "print(f\"Total vectors in index: {stats['total_vector_count']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-pipeline-airflow-3n8gKQVQ-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
